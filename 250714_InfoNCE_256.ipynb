{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNdE2vqpZJyWJNuFtOWHQFJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoqiSheng/MoqiSheng.github.io/blob/main/250714_InfoNCE_256.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrQ4NYfU-hPJ",
        "outputId": "c0dadc5a-6bf9-4dac-c3fa-ce863c3316f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLL5puUsD4yK",
        "outputId": "420541ee-a30d-4514-902a-ebce2af59efd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.7.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "class InfoNCELoss(nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super(InfoNCELoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, anchor, positive, negatives):\n",
        "        batch_size = anchor.size(0)\n",
        "        device = anchor.device\n",
        "\n",
        "        # 计算相似度\n",
        "        pos_sim = F.cosine_similarity(anchor, positive, dim=1) / self.temperature  # [batch_size]\n",
        "\n",
        "        # 负样本相似度\n",
        "        anchor_expanded = anchor.unsqueeze(1)  # [batch_size, 1, feature_dim]\n",
        "        neg_sim = F.cosine_similarity(anchor_expanded, negatives, dim=2) / self.temperature  # [batch_size, num_negatives]\n",
        "\n",
        "        # 拼接正负样本相似度\n",
        "        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)  # [batch_size, 1+num_negatives]\n",
        "\n",
        "        # 标签：正样本索引为0\n",
        "        labels = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "        # 计算交叉熵损失\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def compute_node_similarities(embeddings, batch_size=1000):\n",
        "    num_nodes = embeddings.shape[0]\n",
        "    device = embeddings.device\n",
        "\n",
        "    # 转换为numpy进行相似度计算（CPU上更稳定）\n",
        "    embeddings_cpu = embeddings.detach().cpu().numpy()\n",
        "\n",
        "    # 分批计算相似度\n",
        "    similarity_matrix = np.zeros((num_nodes, num_nodes))\n",
        "\n",
        "    for i in range(0, num_nodes, batch_size):\n",
        "        end_i = min(i + batch_size, num_nodes)\n",
        "        batch_embeddings = embeddings_cpu[i:end_i]\n",
        "\n",
        "        # 计算当前批次与所有节点的相似度\n",
        "        batch_similarities = cosine_similarity(batch_embeddings, embeddings_cpu)\n",
        "        similarity_matrix[i:end_i] = batch_similarities\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def find_positive_negative_samples(similarity_matrix,\n",
        "                                 neg_min_sim=0.1, neg_max_sim=0.5, max_negatives=768,\n",
        "                                 pos_min_sim=0.9, max_positives=50, random_seed=42):\n",
        "    \"\"\"\n",
        "    同时计算正样本和负样本索引\n",
        "\n",
        "    Args:\n",
        "        similarity_matrix: 相似度矩阵\n",
        "        neg_min_sim, neg_max_sim: 负样本相似度范围\n",
        "        max_negatives: 最大负样本数量\n",
        "        pos_min_sim: 正样本最小相似度阈值\n",
        "        max_positives: 最大正样本数量\n",
        "        random_seed: 随机种子\n",
        "\n",
        "    Returns:\n",
        "        negative_indices: 负样本索引字典\n",
        "        positive_indices: 正样本索引字典\n",
        "    \"\"\"\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    num_nodes = similarity_matrix.shape[0]\n",
        "    negative_indices = {}\n",
        "    positive_indices = {}\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        sim_row = similarity_matrix[i]\n",
        "\n",
        "        # 找负样本：相似度在[neg_min_sim, neg_max_sim]范围内\n",
        "        neg_mask = (sim_row >= neg_min_sim) & (sim_row <= neg_max_sim)\n",
        "        neg_candidates = np.where(neg_mask)[0]\n",
        "        neg_candidates = neg_candidates[neg_candidates != i]  # 排除自己\n",
        "\n",
        "        if len(neg_candidates) > max_negatives:\n",
        "            neg_candidates = np.random.choice(neg_candidates, max_negatives, replace=False)\n",
        "\n",
        "        negative_indices[i] = neg_candidates.tolist()\n",
        "\n",
        "        # 找正样本：相似度大于pos_min_sim\n",
        "        pos_mask = sim_row > pos_min_sim\n",
        "        pos_candidates = np.where(pos_mask)[0]\n",
        "        pos_candidates = pos_candidates[pos_candidates != i]  # 排除自己\n",
        "\n",
        "        if len(pos_candidates) > max_positives:\n",
        "            pos_candidates = np.random.choice(pos_candidates, max_positives, replace=False)\n",
        "\n",
        "        positive_indices[i] = pos_candidates.tolist()\n",
        "\n",
        "    return negative_indices, positive_indices\n",
        "\n",
        "\n",
        "def get_positive_samples(embeddings, edge_index, node_indices, positive_indices=None, num_extra_positives=3, random_seed=42):\n",
        "    \"\"\"\n",
        "    获取正样本，包括邻居节点和额外的高相似度正样本\n",
        "\n",
        "    Args:\n",
        "        embeddings: 节点嵌入\n",
        "        edge_index: 边索引\n",
        "        node_indices: 节点索引列表\n",
        "        positive_indices: 额外正样本索引字典\n",
        "        num_extra_positives: 额外正样本数量\n",
        "        random_seed: 随机种子\n",
        "    \"\"\"\n",
        "    device = embeddings.device\n",
        "    positive_samples = []\n",
        "\n",
        "    # 设置随机种子\n",
        "    np.random.seed(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    # 构建邻接列表\n",
        "    num_nodes = embeddings.size(0)\n",
        "    adj_list = {i: [] for i in range(num_nodes)}\n",
        "\n",
        "    for i in range(edge_index.size(1)):\n",
        "        src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
        "        adj_list[src].append(dst)\n",
        "\n",
        "    for node_id in node_indices:\n",
        "        # 获取原始邻居节点\n",
        "        neighbors = adj_list[node_id]\n",
        "        neighbor_embeddings_list = []\n",
        "\n",
        "        if len(neighbors) > 0:\n",
        "            neighbor_embeddings_list.append(embeddings[neighbors])  # [num_neighbors, feature_dim]\n",
        "\n",
        "        # 添加额外的高相似度正样本（如果有的话）\n",
        "        if positive_indices is not None and node_id in positive_indices:\n",
        "            extra_pos_candidates = positive_indices[node_id]\n",
        "\n",
        "            # 从候选中排除已经连接的邻居节点\n",
        "            extra_pos_candidates = [idx for idx in extra_pos_candidates if idx not in neighbors]\n",
        "\n",
        "            if len(extra_pos_candidates) > 0:\n",
        "                # 随机选择额外的正样本\n",
        "                num_to_select = min(num_extra_positives, len(extra_pos_candidates))\n",
        "                selected_extra_pos = np.random.choice(extra_pos_candidates, num_to_select, replace=False)\n",
        "                neighbor_embeddings_list.append(embeddings[selected_extra_pos])\n",
        "\n",
        "        # 计算正样本的平均嵌入\n",
        "        if len(neighbor_embeddings_list) > 0:\n",
        "            all_neighbor_embeddings = torch.cat(neighbor_embeddings_list, dim=0)\n",
        "            positive_sample = torch.mean(all_neighbor_embeddings, dim=0)\n",
        "        else:\n",
        "            # 如果没有邻居和额外正样本，使用自身嵌入\n",
        "            positive_sample = embeddings[node_id]\n",
        "\n",
        "        positive_samples.append(positive_sample)\n",
        "\n",
        "    return torch.stack(positive_samples)  # [len(node_indices), feature_dim]\n",
        "\n",
        "\n",
        "def get_negative_samples(embeddings, negative_indices, node_indices, num_negatives=256, random_seed=42):\n",
        "    \"\"\"获取负样本\"\"\"\n",
        "    np.random.seed(random_seed)\n",
        "    negative_samples = []\n",
        "\n",
        "    for node_id in node_indices:\n",
        "        if node_id in negative_indices and len(negative_indices[node_id]) > 0:\n",
        "            neg_indices = negative_indices[node_id]\n",
        "\n",
        "            # 随机选择负样本\n",
        "            if len(neg_indices) >= num_negatives:\n",
        "                selected_negatives = np.random.choice(neg_indices, num_negatives, replace=False)\n",
        "            else:\n",
        "                selected_negatives = neg_indices + np.random.choice(neg_indices,\n",
        "                                                                  num_negatives - len(neg_indices),\n",
        "                                                                  replace=True).tolist()\n",
        "\n",
        "            neg_embeddings = embeddings[selected_negatives]  # [num_negatives, feature_dim]\n",
        "        else:\n",
        "            # 如果没有负样本，随机选择\n",
        "            num_nodes = embeddings.size(0)\n",
        "            random_indices = np.random.choice(num_nodes, num_negatives, replace=False)\n",
        "            neg_embeddings = embeddings[random_indices]\n",
        "\n",
        "        negative_samples.append(neg_embeddings)\n",
        "\n",
        "    return torch.stack(negative_samples)  # [len(node_indices), num_negatives, feature_dim]\n",
        "\n",
        "\n",
        "def compute_infonce_loss_batch(embeddings, edge_index, negative_indices, positive_indices,\n",
        "                              node_batch, infonce_criterion, num_negatives=256, num_extra_positives=3, epoch=0):\n",
        "    \"\"\"\n",
        "    计算InfoNCE损失（批处理版本）\n",
        "\n",
        "    Args:\n",
        "        embeddings: 节点嵌入\n",
        "        edge_index: 边索引\n",
        "        negative_indices: 负样本索引\n",
        "        positive_indices: 正样本索引\n",
        "        node_batch: 节点批次\n",
        "        infonce_criterion: InfoNCE损失函数\n",
        "        num_negatives: 负样本数量\n",
        "        num_extra_positives: 额外正样本数量\n",
        "        epoch: 当前epoch（用作随机种子的一部分）\n",
        "    \"\"\"\n",
        "    # 使用epoch作为随机种子的一部分，确保可复现性\n",
        "    random_seed = 42 + epoch\n",
        "\n",
        "    # 获取锚点嵌入\n",
        "    anchor_embeddings = embeddings[node_batch]  # [batch_size, feature_dim]\n",
        "\n",
        "    # 获取正样本（包括额外的高相似度正样本）\n",
        "    positive_embeddings = get_positive_samples(embeddings, edge_index, node_batch,\n",
        "                                             positive_indices, num_extra_positives, random_seed)\n",
        "\n",
        "    # 获取负样本\n",
        "    negative_embeddings = get_negative_samples(embeddings, negative_indices,\n",
        "                                             node_batch, num_negatives, random_seed)\n",
        "\n",
        "    # 计算InfoNCE损失\n",
        "    batch_loss = infonce_criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
        "\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "eJ8FU4pADODp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import collections\n",
        "import torch.nn.functional as F\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error, \\\n",
        "    top_k_accuracy_score, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import AutoModel\n",
        "from torch_geometric.nn import GATConv, GCNConv\n",
        "from collections import Counter\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class SVFeatureBlock(nn.Module):\n",
        "    def __init__(self, input_size=512, hidden_size=512, mode='mean'):\n",
        "        super(SVFeatureBlock, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if mode == 'lstm':\n",
        "            self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "            nn.init.orthogonal_(self.lstm.weight_ih_l0)\n",
        "            nn.init.orthogonal_(self.lstm.weight_hh_l0)\n",
        "        elif mode == 'bi-lstm':\n",
        "            self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True,\n",
        "                                bidirectional=True)\n",
        "        elif self.mode == \"gru\":\n",
        "            self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "        elif mode == 'rnn':\n",
        "            self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "    def forward(self, sv):\n",
        "        sv_list = []\n",
        "        for x_tmp in sv:\n",
        "            if self.mode == \"mean\":\n",
        "                if x_tmp.dim() != 1:\n",
        "                    out_put = torch.mean(x_tmp, dim=0)\n",
        "            elif self.mode == \"sum\":\n",
        "                if x_tmp.dim() != 1:\n",
        "                    out_put = torch.sum(x_tmp, dim=0)\n",
        "            elif self.mode == \"max\":\n",
        "                if x_tmp.dim() != 1:\n",
        "                    out_put = torch.max(x_tmp, dim=0).values\n",
        "            elif self.mode == \"lstm\":\n",
        "                out_put, (h_n, c_n) = self.lstm(x_tmp.view(1, -1, self.input_size))\n",
        "                out_put = out_put[:, -1, :]\n",
        "                out_put = torch.squeeze(out_put)\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "            sv_list.append(out_put)\n",
        "        x = torch.stack(sv_list)  # 拼接,(batch,512)\n",
        "        return x\n",
        "\n",
        "\n",
        "def weights_init_1(m):\n",
        "    seed = 20\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "\n",
        "\n",
        "def weights_init_2(m):\n",
        "    seed = 20\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "    torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class Attention_Soft(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size=32):\n",
        "        super(Attention_Soft, self).__init__()\n",
        "\n",
        "        self.l1 = torch.nn.Linear(in_size, hidden_size, bias=True)\n",
        "        self.ac = nn.Sigmoid()\n",
        "        self.l2 = torch.nn.Linear(in_size, hidden_size, bias=False)\n",
        "        self.l3 = torch.nn.Linear(int(hidden_size), 1, bias=False)\n",
        "\n",
        "        weights_init_2(self.l1)\n",
        "        weights_init_1(self.l2)\n",
        "        weights_init_1(self.l3)\n",
        "\n",
        "    def forward(self, z):\n",
        "        w1 = self.l1(torch.mean(z, dim=1).unsqueeze(1))\n",
        "        w2 = self.l2(z)\n",
        "        w = self.ac(w1 + w2)\n",
        "        w = self.l3(w)\n",
        "        beta = torch.softmax(w, dim=1)\n",
        "\n",
        "        return (beta * z).sum(1)\n",
        "\n",
        "\n",
        "class Text_MLP(nn.Module):\n",
        "    def __init__(self, input_dim=4096, hidden_dim=2048, output_dim=768):\n",
        "        super(Text_MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "        weights_init_2(self.layer1)\n",
        "        weights_init_2(self.layer2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SV_GAT(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(SV_GAT, self).__init__()\n",
        "        self.args = args\n",
        "        self.length = list(np.load('/content/drive/MyDrive/USPM_edege_add/data/length.npy'))\n",
        "        pretrain_sv_path = args.pretrain_sv_path\n",
        "        pretrain_scn_path = args.pretrain_scn_path\n",
        "        self.sv_embedding = torch.load(pretrain_sv_path, map_location=torch.device(args.device))\n",
        "        self.scn_embedding = torch.load(pretrain_scn_path, map_location=torch.device(args.device))\n",
        "\n",
        "        self.text_mlp = Text_MLP(input_dim=4096, hidden_dim=2048, output_dim=768)\n",
        "\n",
        "        self.sv_agg = SVFeatureBlock(input_size=768, hidden_size=768, mode=args.mode)\n",
        "\n",
        "        self.attention_soft = Attention_Soft(in_size=768)\n",
        "\n",
        "        self.gat = GAT(input_dim=768, hidden_dim=64, output_dim=10, heads=8, args=args, drop=0.6)\n",
        "\n",
        "        self.gat_poi = GAT_P(input_dim=768, hidden_dim=64, output_dim=4, heads=8, args=args)\n",
        "\n",
        "    def forward(self, epoch=0, test_results=None):\n",
        "        sv_features = self.sv_embedding\n",
        "        street_list = list(torch.split(sv_features, self.length, dim=0))\n",
        "        sv_aggre = self.sv_agg(street_list)\n",
        "        sv_embedding = sv_aggre\n",
        "        scn_embedding = self.text_mlp(self.scn_embedding)  # Reduce text embedding to 768 dim\n",
        "        street_embedding = self.attention_soft(torch.stack([scn_embedding, sv_embedding], dim=1))\n",
        "\n",
        "        if self.args.downstream == 'poi':\n",
        "            gat_loss, infonce_loss, out = self.gat_poi(street_embedding, epoch, test_results)\n",
        "        else:\n",
        "            gat_loss, infonce_loss, s_emb1, out = self.gat(street_embedding, epoch, test_results)\n",
        "\n",
        "        return gat_loss, infonce_loss, out, street_embedding\n",
        "\n",
        "    def test(self, out):\n",
        "        if self.args.downstream == 'poi':\n",
        "            acc, f1_score_test, mrr_test, num, pred_out = self.gat_poi.test(out)\n",
        "            return acc, f1_score_test, mrr_test, 1, 1, 1, num, pred_out\n",
        "        else:\n",
        "            a1, a3, a5, a10, f1, mrr, num, pred_out = self.gat.test(out)\n",
        "            return a1, a3, a5, a10, f1, mrr, num, pred_out\n",
        "\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads, args, drop=0.6):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(0)\n",
        "        self.args = args\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.6)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, output_dim, concat=False, heads=10, dropout=0.6)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "        self.drop1 = nn.Dropout(p=drop)\n",
        "        self.drop2 = nn.Dropout(p=0.6)\n",
        "\n",
        "        # InfoNCE相关\n",
        "        self.infonce_criterion = InfoNCELoss(temperature=0.5)\n",
        "        self.negative_indices = None\n",
        "        self.positive_indices = None  # 新增正样本索引\n",
        "        self.infonce_triggered = False\n",
        "        self.similarity_computed = False\n",
        "        self.result_dir = 'infonce_data'\n",
        "\n",
        "        # 触发条件：num=10且f1>=0.44\n",
        "        self.trigger_num_threshold = 10\n",
        "        self.trigger_f1_threshold = 0.44\n",
        "\n",
        "        self.edge_index = torch.load('/content/drive/MyDrive/USPM_edege_add/data/edge_index.pt').t().contiguous().to(args.device)\n",
        "        self.y = torch.from_numpy(np.load('/content/drive/MyDrive/USPM_edege_add/data/function/label_all_function.npy', allow_pickle=True)).long().to(args.device)\n",
        "        self.train_mask = torch.from_numpy(np.load('/content/drive/MyDrive/USPM_edege_add/data/function/label_mask.npy', allow_pickle=True)).to(args.device)\n",
        "        self.mask = torch.load('/content/drive/MyDrive/USPM_edege_add/data/function/test_mask.pt')\n",
        "        self.y_testlabel = np.load('/content/drive/MyDrive/USPM_edege_add/data/function/label_all_function.npy')[self.mask]\n",
        "\n",
        "    def forward(self, street_embedding, epoch=0, test_results=None):\n",
        "        street_embedding_0 = self.drop1(street_embedding)\n",
        "        street_embedding_1 = self.conv1(street_embedding_0, self.edge_index)\n",
        "        street_embedding_2 = self.elu(street_embedding_1)\n",
        "        street_embedding_2 = self.drop2(street_embedding_2)\n",
        "        street_embedding_2 = self.conv2(street_embedding_2, self.edge_index)\n",
        "\n",
        "        cross_criterion = torch.nn.CrossEntropyLoss()\n",
        "        loss_su = cross_criterion(street_embedding_2[self.train_mask], self.y[self.train_mask])\n",
        "\n",
        "        # InfoNCE损失计算\n",
        "        infonce_loss = torch.tensor(0.0, device=street_embedding.device)\n",
        "\n",
        "        # 检查是否满足触发条件\n",
        "        if test_results and not self.similarity_computed:\n",
        "            num = test_results.get('num', 0)\n",
        "            f1 = test_results.get('f1', 0.0)\n",
        "\n",
        "            if num >= self.trigger_num_threshold and f1 >= self.trigger_f1_threshold:\n",
        "                # 满足条件，计算并保存正负样本索引\n",
        "                print(f\"Triggering InfoNCE for GAT at epoch {epoch}: num={num}, f1={f1:.4f}\")\n",
        "                print(\"Computing positive and negative samples...\")\n",
        "                similarity_matrix = compute_node_similarities(street_embedding, batch_size=500)\n",
        "\n",
        "                # 同时计算正负样本索引\n",
        "                self.negative_indices, self.positive_indices = find_positive_negative_samples(\n",
        "                    similarity_matrix,\n",
        "                    neg_min_sim=0.1, neg_max_sim=0.5, max_negatives=768,\n",
        "                    pos_min_sim=0.9, max_positives=50,\n",
        "                    random_seed=42 + epoch\n",
        "                )\n",
        "\n",
        "                self.similarity_computed = True\n",
        "                self.infonce_triggered = True\n",
        "\n",
        "        if self.infonce_triggered and self.negative_indices is not None and self.positive_indices is not None:\n",
        "            # 已经触发InfoNCE，计算损失\n",
        "            num_nodes = street_embedding_1.size(0)\n",
        "            batch_size = 256  # 分批处理\n",
        "            total_infonce_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            for i in range(0, num_nodes, batch_size):\n",
        "                end_idx = min(i + batch_size, num_nodes)\n",
        "                node_batch = list(range(i, end_idx))\n",
        "\n",
        "                try:\n",
        "                    batch_loss = compute_infonce_loss_batch(\n",
        "                        street_embedding_1, self.edge_index,\n",
        "                        self.negative_indices, self.positive_indices,\n",
        "                        node_batch, self.infonce_criterion,\n",
        "                        num_negatives=256, num_extra_positives=3, epoch=epoch\n",
        "                    )\n",
        "                    total_infonce_loss += batch_loss\n",
        "                    num_batches += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in InfoNCE computation for batch {i}-{end_idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if num_batches > 0:\n",
        "                infonce_loss = total_infonce_loss / num_batches\n",
        "\n",
        "        return loss_su, infonce_loss, street_embedding_1, street_embedding_2\n",
        "\n",
        "    def test(self, out):\n",
        "        pred = out.argmax(dim=1)\n",
        "        pred = pd.DataFrame({'Type': torch.Tensor.cpu(pred).numpy()})\n",
        "\n",
        "        predictions_test_dim = torch.Tensor.cpu(out[self.mask]).argmax(dim=1).detach().numpy()\n",
        "        predictions_test = torch.Tensor.cpu(out[self.mask]).detach().numpy()\n",
        "\n",
        "        A1 = top_k_accuracy_score(self.y_testlabel, predictions_test, k=1, labels=range(10))\n",
        "        A3 = top_k_accuracy_score(self.y_testlabel, predictions_test, k=3, labels=range(10))\n",
        "        A5 = top_k_accuracy_score(self.y_testlabel, predictions_test, k=5, labels=range(10))\n",
        "        print(f'A1={A1}\\t A3={A3}\\t A5={A5} ')\n",
        "\n",
        "        precision_score_test = precision_score(self.y_testlabel, predictions_test_dim, average=\"weighted\")\n",
        "        f1_score_test = f1_score(self.y_testlabel, predictions_test_dim, average=\"weighted\")\n",
        "        mrr_test = compute_mrr(self.y_testlabel, predictions_test)\n",
        "        result = Counter(pred['Type'].values.tolist())\n",
        "        num = len(result)\n",
        "        print(\n",
        "            f'precision={precision_score_test}, f1={f1_score_test}, mrr={mrr_test},num={num}')\n",
        "\n",
        "        print(result)\n",
        "        return A1, A3, A5, 1, f1_score_test, mrr_test, num, out\n",
        "\n",
        "\n",
        "class GAT_P(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads, args, drop=0.6):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=drop)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, output_dim, concat=False, heads=4, dropout=drop)\n",
        "        self.elu = nn.ELU()\n",
        "        self.drop1 = nn.Dropout(p=drop)\n",
        "        self.drop2 = nn.Dropout(p=drop)\n",
        "\n",
        "        # InfoNCE相关\n",
        "        self.infonce_criterion = InfoNCELoss(temperature=0.5)\n",
        "        self.negative_indices = None\n",
        "        self.positive_indices = None  # 新增正样本索引\n",
        "        self.infonce_triggered = False\n",
        "        self.similarity_computed = False\n",
        "        self.result_dir = 'infonce_data'\n",
        "\n",
        "        # 触发条件：num=4且f1>=0.38\n",
        "        self.trigger_num_threshold = 4\n",
        "        self.trigger_f1_threshold = 0.38\n",
        "\n",
        "        self.edge_index = torch.load('/content/drive/MyDrive/USPM_edege_add/data/edge_index.pt').t().contiguous().to(args.device)\n",
        "\n",
        "        self.y = torch.from_numpy(np.load('/content/drive/MyDrive/USPM_edege_add/data/poi/label_all_poi_level.npy', allow_pickle=True)).long().to(args.device)\n",
        "        self.train_mask = torch.from_numpy(np.load('/content/drive/MyDrive/USPM_edege_add/data/poi/label_mask_poi_level.npy', allow_pickle=True)).to(args.device)\n",
        "        self.test_mask = torch.from_numpy(np.load('/content/drive/MyDrive/USPM_edege_add/data/poi/test_mask_poi_level.npy', allow_pickle=True)).to(args.device)\n",
        "\n",
        "        self.mask = torch.from_numpy(np.load('/content/drive/MyDrive/USPM_edege_add/data/poi/test_mask_poi_level.npy', allow_pickle=True))\n",
        "\n",
        "        self.y_testlabel = np.load('/content/drive/MyDrive/USPM_edege_add/data/poi/label_all_poi_level.npy')[self.mask]\n",
        "\n",
        "    def forward(self, street_embedding, epoch=0, test_results=None):\n",
        "        street_embedding_0 = self.drop1(street_embedding)\n",
        "        street_embedding_1 = self.conv1(street_embedding_0, self.edge_index)\n",
        "        street_embedding_2 = self.elu(street_embedding_1)\n",
        "        street_embedding_2 = self.drop2(street_embedding_2)\n",
        "        street_embedding_2 = self.conv2(street_embedding_2, self.edge_index)\n",
        "\n",
        "        cross_criterion = torch.nn.CrossEntropyLoss()\n",
        "        loss_su = cross_criterion(street_embedding_2[self.train_mask], self.y[self.train_mask])\n",
        "\n",
        "        # InfoNCE损失计算\n",
        "        infonce_loss = torch.tensor(0.0, device=street_embedding.device)\n",
        "\n",
        "        # 检查是否满足触发条件\n",
        "        if test_results and not self.similarity_computed:\n",
        "            num = test_results.get('num', 0)\n",
        "            f1 = test_results.get('f1', 0.0)\n",
        "\n",
        "            if num >= self.trigger_num_threshold and f1 >= self.trigger_f1_threshold:\n",
        "                # 满足条件，计算并保存正负样本索引\n",
        "                print(f\"Triggering InfoNCE for GAT_P at epoch {epoch}: num={num}, f1={f1:.4f}\")\n",
        "                print(\"Computing positive and negative samples...\")\n",
        "                similarity_matrix = compute_node_similarities(street_embedding, batch_size=500)\n",
        "\n",
        "                # 同时计算正负样本索引\n",
        "                self.negative_indices, self.positive_indices = find_positive_negative_samples(\n",
        "                    similarity_matrix,\n",
        "                    neg_min_sim=0.1, neg_max_sim=0.5, max_negatives=768,\n",
        "                    pos_min_sim=0.9, max_positives=50,\n",
        "                    random_seed=42 + epoch\n",
        "                )\n",
        "\n",
        "\n",
        "                self.similarity_computed = True\n",
        "                self.infonce_triggered = True\n",
        "\n",
        "        if self.infonce_triggered and self.negative_indices is not None and self.positive_indices is not None:\n",
        "            # 已经触发InfoNCE，计算损失\n",
        "            num_nodes = street_embedding_1.size(0)\n",
        "            batch_size = 256  # 分批处理\n",
        "            total_infonce_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            for i in range(0, num_nodes, batch_size):\n",
        "                end_idx = min(i + batch_size, num_nodes)\n",
        "                node_batch = list(range(i, end_idx))\n",
        "\n",
        "                try:\n",
        "                    batch_loss = compute_infonce_loss_batch(\n",
        "                        street_embedding_1, self.edge_index,\n",
        "                        self.negative_indices, self.positive_indices,\n",
        "                        node_batch, self.infonce_criterion,\n",
        "                        num_negatives=256, num_extra_positives=3, epoch=epoch\n",
        "                    )\n",
        "                    total_infonce_loss += batch_loss\n",
        "                    num_batches += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in InfoNCE computation for batch {i}-{end_idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if num_batches > 0:\n",
        "                infonce_loss = total_infonce_loss / num_batches\n",
        "\n",
        "        return loss_su, infonce_loss, street_embedding_2\n",
        "\n",
        "    def test(self, out):\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct = pred[self.test_mask] == self.y[self.test_mask]\n",
        "        acc = int(correct.sum()) / int(self.test_mask.sum())\n",
        "\n",
        "        pred = pd.DataFrame({'Type': torch.Tensor.cpu(pred).numpy()})\n",
        "\n",
        "        predictions_test_dim = torch.Tensor.cpu(out[self.mask]).argmax(dim=1).detach().numpy()\n",
        "        predictions_test = torch.Tensor.cpu(out[self.mask]).detach().numpy()\n",
        "        f1_score_test = f1_score(self.y_testlabel, predictions_test_dim, average=\"macro\")\n",
        "        mrr_test = compute_mrr(self.y_testlabel, predictions_test)\n",
        "        result = Counter(pred['Type'].values.tolist())\n",
        "        num = len(result)\n",
        "        print(\n",
        "            f'acc={acc}, f1={f1_score_test}, mrr={mrr_test},num={num}')\n",
        "\n",
        "        print(result)\n",
        "        return acc, f1_score_test, mrr_test, num, out\n",
        "\n",
        "\n",
        "def compute_mrr(true_labels, machine_preds):\n",
        "    \"\"\"Compute the MRR \"\"\"\n",
        "    rr_total = 0.0\n",
        "    for i in range(len(true_labels)):\n",
        "        if true_labels[i] == 403:\n",
        "            continue\n",
        "        ranklist = list(np.argsort(machine_preds[i])[::-1])\n",
        "        rank = ranklist.index(true_labels[i]) + 1\n",
        "        rr_total = rr_total + 1.0 / rank\n",
        "    mrr = rr_total / len(true_labels)\n",
        "    return mrr"
      ],
      "metadata": {
        "id": "4yb_qZ2CDUah"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "# from model import SV_GAT\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "parser = argparse.ArgumentParser()\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "\n",
        "parser.add_argument('--device', type=str, default='cuda:0', help='gpu device ids')\n",
        "parser.add_argument('--print_num', type=int, default=1, help='gap of print evaluations')\n",
        "parser.add_argument(\"--print_epoch\", type=int, default=0, help=\"Start print epoch\")\n",
        "parser.add_argument(\"--start_epoch\", type=int, default=0, help=\"Start epoch\")\n",
        "parser.add_argument(\"--current_epoch\", type=int, default=0, help=\"Current epoch\")\n",
        "parser.add_argument(\"--epochs\", type=int, default=200, help=\"Epochs\")\n",
        "parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed.\")\n",
        "parser.add_argument(\"--rounds\", type=int, default=5, help=\"number of training rounds\")\n",
        "parser.add_argument(\"--mode\", type=str, default='lstm', help=\"aggression function.\")\n",
        "\n",
        "args = parser.parse_args([])\n",
        "\n",
        "def trainer(args, model, optimizer1, optimizer2, optimizer3, optimizer4, epoch, test_results=None):\n",
        "    loss_epoch = []\n",
        "    loss_su_epoch = []\n",
        "    infonce_loss_epoch = []\n",
        "    total_loss_epoch = []\n",
        "\n",
        "    model.train()\n",
        "    optimizer1.zero_grad() # attention\n",
        "    optimizer2.zero_grad() # sv_agg(lstm)\n",
        "    optimizer3.zero_grad() # gat\n",
        "    optimizer4.zero_grad() # text_mlp\n",
        "\n",
        "    # 传入epoch和test_results参数以便模型检查InfoNCE触发条件\n",
        "    gnn_loss, infonce_loss, pre_out, street_embedding = model(epoch, test_results)\n",
        "\n",
        "    # 记录各种损失\n",
        "    loss_su_epoch.append(gnn_loss.item())\n",
        "    infonce_loss_epoch.append(infonce_loss.item())\n",
        "\n",
        "    # 计算总损失\n",
        "    if args.downstream == 'poi':\n",
        "        infonce_triggered = model.gat_poi.infonce_triggered\n",
        "    else:\n",
        "        infonce_triggered = model.gat.infonce_triggered\n",
        "\n",
        "    if not infonce_triggered:\n",
        "        # InfoNCE未触发前，只使用原始损失\n",
        "        total_loss = gnn_loss\n",
        "    else:\n",
        "        # InfoNCE触发后，使用加权损失\n",
        "        total_loss = 0.8 * gnn_loss + 0.2 * infonce_loss\n",
        "\n",
        "    total_loss_epoch.append(total_loss.item())\n",
        "    loss_epoch.append(total_loss.item())  # 保持兼容性\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer1.step()\n",
        "    optimizer2.step()\n",
        "    optimizer3.step()\n",
        "    optimizer4.step()\n",
        "\n",
        "    if epoch % args.print_num == 0:\n",
        "        if not infonce_triggered:\n",
        "            print(f\"TrainEpoch [{epoch + 1}/{args.epochs}]\\t loss_su:{np.mean(loss_su_epoch):.6f}\")\n",
        "        else:\n",
        "            print(f\"TrainEpoch [{epoch + 1}/{args.epochs}]\\t total_loss:{np.mean(total_loss_epoch):.6f}\\t \"\n",
        "                  f\"loss_su:{np.mean(loss_su_epoch):.6f}\\t infonce_loss:{np.mean(infonce_loss_epoch):.6f}\")\n",
        "\n",
        "    return np.mean(loss_epoch), pre_out, street_embedding\n",
        "\n",
        "def test(args, model, epoch, round_num, result_dir):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        _, _, out, _ = model(epoch)  # 测试时不传入test_results\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        os.makedirs(result_dir, exist_ok=True)\n",
        "        if args.downstream == 'poi':\n",
        "            acc, f1, mrr, _, _, _, num, pred_out = model.test(out)\n",
        "            result = {\n",
        "                'epoch': epoch + 1,\n",
        "                'acc': acc,\n",
        "                'f1': f1,\n",
        "                'mrr': mrr,\n",
        "                'num': num\n",
        "            }\n",
        "            return acc, f1, mrr, 1, 1, 1, num, pred_out, result\n",
        "        else:\n",
        "            a1, a3, a5, a10, f1, mrr, num, pred_out = model.test(out)\n",
        "            result = {\n",
        "                'epoch': epoch + 1,\n",
        "                'a1': a1,\n",
        "                'a3': a3,\n",
        "                'a5': a5,\n",
        "                'a10': a10,\n",
        "                'f1': f1,\n",
        "                'mrr': mrr,\n",
        "                'num': num\n",
        "            }\n",
        "            return a1, a3, a5, a10, f1, mrr, num, pred_out, result\n",
        "\n",
        "def calculate_best_results(result_dir, downstream):\n",
        "    # Collect round files\n",
        "    round_files = [f for f in os.listdir(result_dir) if f.startswith('round_') and f.endswith('.npy')]\n",
        "    if not round_files:\n",
        "        return None\n",
        "\n",
        "    # Load all results and group by epoch\n",
        "    epoch_results = {}\n",
        "    for round_file in round_files:\n",
        "        round_data = np.load(os.path.join(result_dir, round_file), allow_pickle=True).item()\n",
        "        for epoch_data in round_data['epochs']:\n",
        "            epoch = epoch_data['epoch']\n",
        "            if epoch not in epoch_results:\n",
        "                epoch_results[epoch] = []\n",
        "            epoch_results[epoch].append(epoch_data)\n",
        "\n",
        "    # Compute per-epoch averages across rounds\n",
        "    epoch_averages = {}\n",
        "    best_f1 = -1\n",
        "    best_epoch = None\n",
        "\n",
        "    if downstream == 'poi':\n",
        "        all_results = []\n",
        "        for epoch in epoch_results:\n",
        "            accs = [r['acc'] for r in epoch_results[epoch]]\n",
        "            f1s = [r['f1'] for r in epoch_results[epoch]]\n",
        "            mrrs = [r['mrr'] for r in epoch_results[epoch]]\n",
        "            epoch_averages[epoch] = {\n",
        "                'avg_acc': np.mean(accs),\n",
        "                'avg_f1': np.mean(f1s),\n",
        "                'avg_mrr': np.mean(mrrs)\n",
        "            }\n",
        "            all_results.extend(epoch_results[epoch])\n",
        "            if epoch_averages[epoch]['avg_f1'] > best_f1:\n",
        "                best_f1 = epoch_averages[epoch]['avg_f1']\n",
        "                best_epoch = epoch\n",
        "\n",
        "        # Store best epoch results and overall max metrics\n",
        "        best_results = {\n",
        "            'best_epoch': best_epoch,\n",
        "            'best_epoch_avg_acc': epoch_averages[best_epoch]['avg_acc'],\n",
        "            'best_epoch_avg_f1': epoch_averages[best_epoch]['avg_f1'],\n",
        "            'best_epoch_avg_mrr': epoch_averages[best_epoch]['avg_mrr'],\n",
        "            'overall_best_acc': max([r['acc'] for r in all_results]),\n",
        "            'overall_best_f1': max([r['f1'] for r in all_results]),\n",
        "            'overall_best_mrr': max([r['mrr'] for r in all_results])\n",
        "        }\n",
        "    else:\n",
        "        all_results = []\n",
        "        for epoch in epoch_results:\n",
        "            a1s = [r['a1'] for r in epoch_results[epoch]]\n",
        "            a3s = [r['a3'] for r in epoch_results[epoch]]\n",
        "            a5s = [r['a5'] for r in epoch_results[epoch]]\n",
        "            a10s = [r['a10'] for r in epoch_results[epoch]]\n",
        "            f1s = [r['f1'] for r in epoch_results[epoch]]\n",
        "            mrrs = [r['mrr'] for r in epoch_results[epoch]]\n",
        "            epoch_averages[epoch] = {\n",
        "                'avg_a1': np.mean(a1s),\n",
        "                'avg_a3': np.mean(a3s),\n",
        "                'avg_a5': np.mean(a5s),\n",
        "                'avg_a10': np.mean(a10s),\n",
        "                'avg_f1': np.mean(f1s),\n",
        "                'avg_mrr': np.mean(mrrs)\n",
        "            }\n",
        "            all_results.extend(epoch_results[epoch])\n",
        "            if epoch_averages[epoch]['avg_f1'] > best_f1:\n",
        "                best_f1 = epoch_averages[epoch]['avg_f1']\n",
        "                best_epoch = epoch\n",
        "\n",
        "        # Store best epoch results and overall max metrics\n",
        "        best_results = {\n",
        "            'best_epoch': best_epoch,\n",
        "            'best_epoch_avg_a1': epoch_averages[best_epoch]['avg_a1'],\n",
        "            'best_epoch_avg_a3': epoch_averages[best_epoch]['avg_a3'],\n",
        "            'best_epoch_avg_a5': epoch_averages[best_epoch]['avg_a5'],\n",
        "            'best_epoch_avg_a10': epoch_averages[best_epoch]['avg_a10'],\n",
        "            'best_epoch_avg_f1': epoch_averages[best_epoch]['avg_f1'],\n",
        "            'best_epoch_avg_mrr': epoch_averages[best_epoch]['avg_mrr'],\n",
        "            'overall_best_a1': max([r['a1'] for r in all_results]),\n",
        "            'overall_best_a3': max([r['a3'] for r in all_results]),\n",
        "            'overall_best_a5': max([r['a5'] for r in all_results]),\n",
        "            'overall_best_a10': max([r['a10'] for r in all_results]),\n",
        "            'overall_best_f1': max([r['f1'] for r in all_results]),\n",
        "            'overall_best_mrr': max([r['mrr'] for r in all_results])\n",
        "        }\n",
        "\n",
        "    # Save per-epoch averages and best results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    np.save(f'{result_dir}/epoch_averages_{timestamp}.npy', epoch_averages)\n",
        "    np.save(f'{result_dir}/best_results_{timestamp}.npy', best_results)\n",
        "    return best_results\n",
        "\n",
        "def run_training(pretrain_sv_path, result_subdir):\n",
        "    result_dir = f'/content/drive/MyDrive/USPM_edege_add/result/{result_subdir}'\n",
        "    os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "    for downstream in ['function', 'poi']:\n",
        "        print(f\"\\nStarting {downstream} downstream with {pretrain_sv_path}\")\n",
        "        args.downstream = downstream\n",
        "        args.pretrain_sv_path = pretrain_sv_path\n",
        "        # Set pretrain_scn_path based on downstream task\n",
        "        if downstream == 'function':\n",
        "            args.pretrain_scn_path = '/content/drive/MyDrive/USPM_edege_add/embeddings/qwen_text_embedding_function_72.pt'\n",
        "        else:  # downstream == 'poi'\n",
        "            args.pretrain_scn_path = '/content/drive/MyDrive/USPM_edege_add/embeddings/qwen_text_embedding_poi_72.pt'\n",
        "        args.current_epoch = 0\n",
        "\n",
        "        # 打印触发条件信息和正负样本配置\n",
        "        if downstream == 'poi':\n",
        "            print(f\"InfoNCE Configuration:\")\n",
        "            print(f\"  - Trigger: num >= 4 and f1 >= 0.38\")\n",
        "            print(f\"  - Negative samples: similarity ∈ [0.1, 0.5], max_negatives=768\")\n",
        "            print(f\"  - Positive samples: similarity > 0.9, max_positives=50\")\n",
        "            print(f\"  - Extra positives per node: 3 (excluding existing neighbors)\")\n",
        "        else:\n",
        "            print(f\"InfoNCE Configuration:\")\n",
        "            print(f\"  - Trigger: num >= 10 and f1 >= 0.44\")\n",
        "            print(f\"  - Negative samples: similarity ∈ [0.1, 0.5], max_negatives=768\")\n",
        "            print(f\"  - Positive samples: similarity > 0.9, max_positives=50\")\n",
        "            print(f\"  - Extra positives per node: 3 (excluding existing neighbors)\")\n",
        "\n",
        "        for round_num in range(args.rounds):\n",
        "        # for round_num in range(4, 5):\n",
        "            print(f\"\\nRound {round_num + 1}/{args.rounds}\")\n",
        "\n",
        "            # 设置随机种子，确保可复现性\n",
        "            base_seed = args.seed + round_num\n",
        "            np.random.seed(base_seed)\n",
        "            random.seed(base_seed + 1)\n",
        "            torch.manual_seed(base_seed + 2)\n",
        "            torch.cuda.manual_seed(base_seed + 3)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            print(f\"Random seeds set: numpy={base_seed}, random={base_seed+1}, torch={base_seed+2}, cuda={base_seed+3}\")\n",
        "\n",
        "            model = SV_GAT(args)\n",
        "            model = model.to(args.device)\n",
        "\n",
        "            opt1 = torch.optim.Adam(\n",
        "                itertools.chain(model.attention_soft.parameters()),\n",
        "                lr=0.0005, weight_decay=1e-8)\n",
        "            opt4 = torch.optim.Adam(\n",
        "                model.text_mlp.parameters(),\n",
        "                lr=0.0005, weight_decay=1e-8)  # MLP和attention一样的设置\n",
        "            if args.downstream == 'poi':\n",
        "                opt3 = torch.optim.Adam(model.gat_poi.parameters(), lr=0.0005, weight_decay=5e-4)\n",
        "                args.epochs = 250\n",
        "            else:\n",
        "                opt3 = torch.optim.Adam(model.gat.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "                args.epochs = 250\n",
        "\n",
        "            if args.mode != 'mean':\n",
        "                opt2 = torch.optim.SGD(model.sv_agg.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
        "                t = 10\n",
        "                T = 800\n",
        "                n_t = 0.5\n",
        "                lf = lambda epoch: (0.9 * epoch / t + 0.1) if epoch < t else 0.1 if n_t * (\n",
        "                        1 + math.cos(math.pi * (epoch - t) / (T - t))) < 0.1 else n_t * (\n",
        "                        1 + math.cos(math.pi * (epoch - t) / (T - t)))\n",
        "                scheduler = torch.optim.lr_scheduler.LambdaLR(opt2, lr_lambda=lf)\n",
        "            else:\n",
        "                opt2 = torch.optim.SGD(model.sv_agg.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
        "\n",
        "            print(model)\n",
        "\n",
        "            # Collect results for this round\n",
        "            round_results = {'round': round_num, 'epochs': []}\n",
        "            last_test_result = None  # 保存上一次测试结果\n",
        "\n",
        "            for epoch in range(args.start_epoch, args.epochs):\n",
        "                # 在训练时传入上一次的测试结果，以便检查InfoNCE触发条件\n",
        "                loss_epoch, pred_, street_embedding = trainer(args, model, opt1, opt2, opt3, opt4, epoch, last_test_result)\n",
        "                if args.mode != 'mean':\n",
        "                    scheduler.step()\n",
        "                if epoch % args.print_num == 0:\n",
        "                    result_tuple = test(args, model, epoch, round_num, f'{result_dir}/{downstream}')\n",
        "                    # Append result to round_results\n",
        "                    round_results['epochs'].append(result_tuple[-1])  # Last element is the result dict\n",
        "                    last_test_result = result_tuple[-1]  # 保存当前测试结果\n",
        "\n",
        "            # Save all results for this round in a single file\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            np.save(f'{result_dir}/{downstream}/round_{round_num}_{timestamp}.npy', round_results)\n",
        "\n",
        "        # Calculate and save average and best results\n",
        "        best_results = calculate_best_results(f'{result_dir}/{downstream}', downstream)\n",
        "        print(f\"Best Results for {downstream}:\", best_results)\n",
        "\n",
        "# 运行训练\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Enhanced InfoNCE with Positive and Negative Sampling\")\n",
        "    print(\"=\"*50)\n",
        "    run_training('/content/drive/MyDrive/USPM_edege_add/embeddings/image_representation_117144_16.pt',\n",
        "                 'infonce_pos3_neg_256_0.2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVedm1LDD-91",
        "outputId": "89c9b0cd-d070-4dc8-ea5e-830dc202d871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced InfoNCE with Positive and Negative Sampling\n",
            "==================================================\n",
            "\n",
            "Starting function downstream with /content/drive/MyDrive/USPM_edege_add/embeddings/image_representation_117144_16.pt\n",
            "InfoNCE Configuration:\n",
            "  - Trigger: num >= 10 and f1 >= 0.44\n",
            "  - Negative samples: similarity ∈ [0.1, 0.5], max_negatives=768\n",
            "  - Positive samples: similarity > 0.9, max_positives=50\n",
            "  - Extra positives per node: 3 (excluding existing neighbors)\n",
            "\n",
            "Round 1/5\n",
            "Random seeds set: numpy=42, random=43, torch=44, cuda=45\n",
            "SV_GAT(\n",
            "  (text_mlp): Text_MLP(\n",
            "    (layer1): Linear(in_features=4096, out_features=2048, bias=True)\n",
            "    (relu): ReLU()\n",
            "    (layer2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "  )\n",
            "  (sv_agg): SVFeatureBlock(\n",
            "    (lstm): LSTM(768, 768, batch_first=True)\n",
            "  )\n",
            "  (attention_soft): Attention_Soft(\n",
            "    (l1): Linear(in_features=768, out_features=32, bias=True)\n",
            "    (ac): Sigmoid()\n",
            "    (l2): Linear(in_features=768, out_features=32, bias=False)\n",
            "    (l3): Linear(in_features=32, out_features=1, bias=False)\n",
            "  )\n",
            "  (gat): GAT(\n",
            "    (conv1): GATConv(768, 64, heads=8)\n",
            "    (conv2): GATConv(512, 10, heads=10)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (drop1): Dropout(p=0.6, inplace=False)\n",
            "    (drop2): Dropout(p=0.6, inplace=False)\n",
            "    (infonce_criterion): InfoNCELoss()\n",
            "  )\n",
            "  (gat_poi): GAT_P(\n",
            "    (conv1): GATConv(768, 64, heads=8)\n",
            "    (conv2): GATConv(512, 4, heads=4)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (drop1): Dropout(p=0.6, inplace=False)\n",
            "    (drop2): Dropout(p=0.6, inplace=False)\n",
            "    (infonce_criterion): InfoNCELoss()\n",
            "  )\n",
            ")\n",
            "TrainEpoch [1/250]\t loss_su:2.886657\n",
            "A1=0.46891342242882045\t A3=0.6649234940925818\t A5=0.7853960875460004 \n",
            "precision=0.21987979773390942, f1=0.2993774777690333, mrr=0.6056363348490017,num=1\n",
            "Counter({0: 5458})\n",
            "TrainEpoch [2/250]\t loss_su:6.231884\n",
            "A1=0.06992058880495836\t A3=0.26573697462715473\t A5=0.34398605461940734 \n",
            "precision=0.017392515343603077, f1=0.009481231573142497, mrr=0.24164576089328843,num=2\n",
            "Counter({2: 5450, 3: 8})\n",
            "TrainEpoch [3/250]\t loss_su:5.716518\n",
            "A1=0.07515010652721286\t A3=0.24249467363935695\t A5=0.3852411388727484 \n",
            "precision=0.005647538511051441, f1=0.010505581456515435, mrr=0.24117753305724637,num=1\n",
            "Counter({3: 5458})\n",
            "TrainEpoch [4/250]\t loss_su:4.155100\n",
            "A1=0.46891342242882045\t A3=0.6655045516172767\t A5=0.7853960875460004 \n",
            "precision=0.21987979773390942, f1=0.2993774777690333, mrr=0.6124330169797949,num=1\n",
            "Counter({0: 5458})\n",
            "TrainEpoch [5/250]\t loss_su:3.772134\n",
            "A1=0.46891342242882045\t A3=0.6595002905287624\t A5=0.7853960875460004 \n",
            "precision=0.21987979773390942, f1=0.2993774777690333, mrr=0.6011614233142452,num=1\n",
            "Counter({0: 5458})\n",
            "TrainEpoch [6/250]\t loss_su:3.577777\n",
            "A1=0.10923881464264962\t A3=0.6674414100329266\t A5=0.7853960875460004 \n",
            "precision=0.2815519428862366, f1=0.08235533563570689, mrr=0.4220052479639904,num=2\n",
            "Counter({2: 5062, 0: 396})\n",
            "TrainEpoch [7/250]\t loss_su:2.715337\n",
            "A1=0.18865000968429207\t A3=0.6137904319194267\t A5=0.6902963393375944 \n",
            "precision=0.23068269540179087, f1=0.17248493633812984, mrr=0.4514141372218091,num=2\n",
            "Counter({3: 4027, 0: 1431})\n",
            "TrainEpoch [8/250]\t loss_su:2.830302\n",
            "A1=0.4685260507456905\t A3=0.5698237458841758\t A5=0.7420104590354445 \n",
            "precision=0.21982588543707687, f1=0.29924853419214265, mrr=0.5919979309433122,num=2\n",
            "Counter({0: 5455, 3: 3})\n",
            "TrainEpoch [9/250]\t loss_su:2.664833\n",
            "A1=0.46891342242882045\t A3=0.6850668216153399\t A5=0.7420104590354445 \n",
            "precision=0.21987979773390942, f1=0.2993774777690333, mrr=0.6126796589899492,num=1\n",
            "Counter({0: 5458})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 检查传入的文件路径\n",
        "file_path = '/content/drive/MyDrive/USPM_edege_add/embeddings/image_representation_117144_16.pt'\n",
        "print(\"目标文件是否存在：\", os.path.exists(file_path))\n",
        "\n",
        "# 检查错误信息中的文件路径\n",
        "error_file = 'embeddings/qwen_text_embedding_function_72.p'\n",
        "print(\"错误文件是否存在：\", os.path.exists(error_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CuZ1FVBHElj",
        "outputId": "69e60c07-919b-4b88-c2dc-951c1d0eab10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "目标文件是否存在： True\n",
            "错误文件是否存在： False\n"
          ]
        }
      ]
    }
  ]
}