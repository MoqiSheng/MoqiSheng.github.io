{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPx3CqWIvlUPsjamvW8oYFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoqiSheng/MoqiSheng.github.io/blob/main/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZJHq3Yx4I-p",
        "outputId": "c2aa820d-48e8-4b2d-e963-5c5b87b7a53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/MyDrive/images/anchor_pool.zip\n",
            "checkdir:  cannot create extraction directory: /content/images/anchor_pool\n",
            "           No such file or directory\n",
            "Archive:  /content/drive/MyDrive/images/candidate_images.zip\n",
            "checkdir:  cannot create extraction directory: /content/images/candidate_images\n",
            "           No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# 连接 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 解压 anchor_pool.zip 到 images/anchor_pool 文件夹\n",
        "!unzip /content/drive/MyDrive/images/anchor_pool.zip -d /content/images/anchor_pool\n",
        "\n",
        "# 解压 candidate_images.zip 到 images/candidate_images 文件夹\n",
        "!unzip /content/drive/MyDrive/images/candidate_images.zip -d /content/images/candidate_images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 在 /content/ 下创建 images 文件夹\n",
        "!mkdir -p /content/images\n",
        "\n",
        "# 解压 anchor_pool.zip 到 /content/images/anchor_pool 文件夹\n",
        "!unzip /content/drive/MyDrive/images/anchor_pool.zip -d /content/images/anchor_pool\n",
        "\n",
        "# 解压 candidate_images.zip 到 /content/images/candidate_images 文件夹\n",
        "!unzip /content/drive/MyDrive/images/candidate_images.zip -d /content/images/candidate_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjrBFdOQ5QnM",
        "outputId": "b52158d9-af28-49e0-f732-ca2c24afb40a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/images/anchor_pool.zip\n",
            "  inflating: /content/images/anchor_pool/1002.png  \n",
            "  inflating: /content/images/anchor_pool/1004.png  \n",
            "  inflating: /content/images/anchor_pool/1010.png  \n",
            "  inflating: /content/images/anchor_pool/1013.png  \n",
            "  inflating: /content/images/anchor_pool/1029.png  \n",
            "  inflating: /content/images/anchor_pool/1034.png  \n",
            "  inflating: /content/images/anchor_pool/1043.png  \n",
            "  inflating: /content/images/anchor_pool/1048.png  \n",
            "  inflating: /content/images/anchor_pool/105.png  \n",
            "  inflating: /content/images/anchor_pool/1052.png  \n",
            "  inflating: /content/images/anchor_pool/1057.png  \n",
            "  inflating: /content/images/anchor_pool/1058.png  \n",
            "  inflating: /content/images/anchor_pool/1059.png  \n",
            "  inflating: /content/images/anchor_pool/1069.png  \n",
            "  inflating: /content/images/anchor_pool/1079.png  \n",
            "  inflating: /content/images/anchor_pool/1086.png  \n",
            "  inflating: /content/images/anchor_pool/1092.png  \n",
            "  inflating: /content/images/anchor_pool/1096.png  \n",
            "  inflating: /content/images/anchor_pool/1097.png  \n",
            "  inflating: /content/images/anchor_pool/1099.png  \n",
            "  inflating: /content/images/anchor_pool/1102.png  \n",
            "  inflating: /content/images/anchor_pool/1104.png  \n",
            "  inflating: /content/images/anchor_pool/1122.png  \n",
            "  inflating: /content/images/anchor_pool/1129.png  \n",
            "  inflating: /content/images/anchor_pool/1139.png  \n",
            "  inflating: /content/images/anchor_pool/114.png  \n",
            "  inflating: /content/images/anchor_pool/1140.png  \n",
            "  inflating: /content/images/anchor_pool/1146.png  \n",
            "  inflating: /content/images/anchor_pool/1152.png  \n",
            "  inflating: /content/images/anchor_pool/1155.png  \n",
            "  inflating: /content/images/anchor_pool/1159.png  \n",
            "  inflating: /content/images/anchor_pool/1166.png  \n",
            "  inflating: /content/images/anchor_pool/1168.png  \n",
            "  inflating: /content/images/anchor_pool/117.png  \n",
            "  inflating: /content/images/anchor_pool/1179.png  \n",
            "  inflating: /content/images/anchor_pool/1193.png  \n",
            "  inflating: /content/images/anchor_pool/1194.png  \n",
            "  inflating: /content/images/anchor_pool/1212.png  \n",
            "  inflating: /content/images/anchor_pool/1216.png  \n",
            "  inflating: /content/images/anchor_pool/1220.png  \n",
            "  inflating: /content/images/anchor_pool/1237.png  \n",
            "  inflating: /content/images/anchor_pool/1241.png  \n",
            "  inflating: /content/images/anchor_pool/1252.png  \n",
            "  inflating: /content/images/anchor_pool/1263.png  \n",
            "  inflating: /content/images/anchor_pool/1267.png  \n",
            "  inflating: /content/images/anchor_pool/1271.png  \n",
            "  inflating: /content/images/anchor_pool/1274.png  \n",
            "  inflating: /content/images/anchor_pool/1276.png  \n",
            "  inflating: /content/images/anchor_pool/1290.png  \n",
            "  inflating: /content/images/anchor_pool/1294.png  \n",
            "  inflating: /content/images/anchor_pool/1299.png  \n",
            "  inflating: /content/images/anchor_pool/1304.png  \n",
            "  inflating: /content/images/anchor_pool/1310.png  \n",
            "  inflating: /content/images/anchor_pool/1313.png  \n",
            "  inflating: /content/images/anchor_pool/1320.png  \n",
            "  inflating: /content/images/anchor_pool/1327.png  \n",
            "  inflating: /content/images/anchor_pool/1330.png  \n",
            "  inflating: /content/images/anchor_pool/1332.png  \n",
            "  inflating: /content/images/anchor_pool/1333.png  \n",
            "  inflating: /content/images/anchor_pool/1338.png  \n",
            "  inflating: /content/images/anchor_pool/1344.png  \n",
            "  inflating: /content/images/anchor_pool/1352.png  \n",
            "  inflating: /content/images/anchor_pool/1373.png  \n",
            "  inflating: /content/images/anchor_pool/1384.png  \n",
            "  inflating: /content/images/anchor_pool/1395.png  \n",
            "  inflating: /content/images/anchor_pool/1398.png  \n",
            "  inflating: /content/images/anchor_pool/140.png  \n",
            "  inflating: /content/images/anchor_pool/1404.png  \n",
            "  inflating: /content/images/anchor_pool/1407.png  \n",
            "  inflating: /content/images/anchor_pool/1409.png  \n",
            "  inflating: /content/images/anchor_pool/1422.png  \n",
            "  inflating: /content/images/anchor_pool/1435.png  \n",
            "  inflating: /content/images/anchor_pool/1445.png  \n",
            "  inflating: /content/images/anchor_pool/1446.png  \n",
            "  inflating: /content/images/anchor_pool/145.png  \n",
            "  inflating: /content/images/anchor_pool/1450.png  \n",
            "  inflating: /content/images/anchor_pool/1455.png  \n",
            "  inflating: /content/images/anchor_pool/1457.png  \n",
            "  inflating: /content/images/anchor_pool/1460.png  \n",
            "  inflating: /content/images/anchor_pool/1468.png  \n",
            "  inflating: /content/images/anchor_pool/1481.png  \n",
            "  inflating: /content/images/anchor_pool/1489.png  \n",
            "  inflating: /content/images/anchor_pool/1491.png  \n",
            "  inflating: /content/images/anchor_pool/1503.png  \n",
            "  inflating: /content/images/anchor_pool/1509.png  \n",
            "  inflating: /content/images/anchor_pool/1510.png  \n",
            "  inflating: /content/images/anchor_pool/154.png  \n",
            "  inflating: /content/images/anchor_pool/158.png  \n",
            "  inflating: /content/images/anchor_pool/171.png  \n",
            "  inflating: /content/images/anchor_pool/172.png  \n",
            "  inflating: /content/images/anchor_pool/209.png  \n",
            "  inflating: /content/images/anchor_pool/214.png  \n",
            "  inflating: /content/images/anchor_pool/215.png  \n",
            "  inflating: /content/images/anchor_pool/22.png  \n",
            "  inflating: /content/images/anchor_pool/246.png  \n",
            "  inflating: /content/images/anchor_pool/249.png  \n",
            "  inflating: /content/images/anchor_pool/250.png  \n",
            "  inflating: /content/images/anchor_pool/259.png  \n",
            "  inflating: /content/images/anchor_pool/268.png  \n",
            "  inflating: /content/images/anchor_pool/269.png  \n",
            "  inflating: /content/images/anchor_pool/271.png  \n",
            "  inflating: /content/images/anchor_pool/276.png  \n",
            "  inflating: /content/images/anchor_pool/278.png  \n",
            "  inflating: /content/images/anchor_pool/285.png  \n",
            "  inflating: /content/images/anchor_pool/300.png  \n",
            "  inflating: /content/images/anchor_pool/305.png  \n",
            "  inflating: /content/images/anchor_pool/318.png  \n",
            "  inflating: /content/images/anchor_pool/327.png  \n",
            "  inflating: /content/images/anchor_pool/329.png  \n",
            "  inflating: /content/images/anchor_pool/332.png  \n",
            "  inflating: /content/images/anchor_pool/349.png  \n",
            "  inflating: /content/images/anchor_pool/353.png  \n",
            "  inflating: /content/images/anchor_pool/362.png  \n",
            "  inflating: /content/images/anchor_pool/369.png  \n",
            "  inflating: /content/images/anchor_pool/37.png  \n",
            "  inflating: /content/images/anchor_pool/385.png  \n",
            "  inflating: /content/images/anchor_pool/386.png  \n",
            "  inflating: /content/images/anchor_pool/396.png  \n",
            "  inflating: /content/images/anchor_pool/400.png  \n",
            "  inflating: /content/images/anchor_pool/402.png  \n",
            "  inflating: /content/images/anchor_pool/407.png  \n",
            "  inflating: /content/images/anchor_pool/408.png  \n",
            "  inflating: /content/images/anchor_pool/412.png  \n",
            "  inflating: /content/images/anchor_pool/425.png  \n",
            "  inflating: /content/images/anchor_pool/426.png  \n",
            "  inflating: /content/images/anchor_pool/430.png  \n",
            "  inflating: /content/images/anchor_pool/431.png  \n",
            "  inflating: /content/images/anchor_pool/442.png  \n",
            "  inflating: /content/images/anchor_pool/445.png  \n",
            "  inflating: /content/images/anchor_pool/449.png  \n",
            "  inflating: /content/images/anchor_pool/456.png  \n",
            "  inflating: /content/images/anchor_pool/463.png  \n",
            "  inflating: /content/images/anchor_pool/47.png  \n",
            "  inflating: /content/images/anchor_pool/487.png  \n",
            "  inflating: /content/images/anchor_pool/490.png  \n",
            "  inflating: /content/images/anchor_pool/491.png  \n",
            "  inflating: /content/images/anchor_pool/492.png  \n",
            "  inflating: /content/images/anchor_pool/50.png  \n",
            "  inflating: /content/images/anchor_pool/518.png  \n",
            "  inflating: /content/images/anchor_pool/523.png  \n",
            "  inflating: /content/images/anchor_pool/525.png  \n",
            "  inflating: /content/images/anchor_pool/526.png  \n",
            "  inflating: /content/images/anchor_pool/538.png  \n",
            "  inflating: /content/images/anchor_pool/541.png  \n",
            "  inflating: /content/images/anchor_pool/544.png  \n",
            "  inflating: /content/images/anchor_pool/561.png  \n",
            "  inflating: /content/images/anchor_pool/569.png  \n",
            "  inflating: /content/images/anchor_pool/578.png  \n",
            "  inflating: /content/images/anchor_pool/585.png  \n",
            "  inflating: /content/images/anchor_pool/596.png  \n",
            "  inflating: /content/images/anchor_pool/601.png  \n",
            "  inflating: /content/images/anchor_pool/613.png  \n",
            "  inflating: /content/images/anchor_pool/625.png  \n",
            "  inflating: /content/images/anchor_pool/627.png  \n",
            "  inflating: /content/images/anchor_pool/63.png  \n",
            "  inflating: /content/images/anchor_pool/633.png  \n",
            "  inflating: /content/images/anchor_pool/644.png  \n",
            "  inflating: /content/images/anchor_pool/650.png  \n",
            "  inflating: /content/images/anchor_pool/651.png  \n",
            "  inflating: /content/images/anchor_pool/653.png  \n",
            "  inflating: /content/images/anchor_pool/663.png  \n",
            "  inflating: /content/images/anchor_pool/666.png  \n",
            "  inflating: /content/images/anchor_pool/672.png  \n",
            "  inflating: /content/images/anchor_pool/68.png  \n",
            "  inflating: /content/images/anchor_pool/681.png  \n",
            "  inflating: /content/images/anchor_pool/690.png  \n",
            "  inflating: /content/images/anchor_pool/697.png  \n",
            "  inflating: /content/images/anchor_pool/698.png  \n",
            "  inflating: /content/images/anchor_pool/702.png  \n",
            "  inflating: /content/images/anchor_pool/704.png  \n",
            "  inflating: /content/images/anchor_pool/708.png  \n",
            "  inflating: /content/images/anchor_pool/726.png  \n",
            "  inflating: /content/images/anchor_pool/736.png  \n",
            "  inflating: /content/images/anchor_pool/747.png  \n",
            "  inflating: /content/images/anchor_pool/753.png  \n",
            "  inflating: /content/images/anchor_pool/757.png  \n",
            "  inflating: /content/images/anchor_pool/765.png  \n",
            "  inflating: /content/images/anchor_pool/77.png  \n",
            "  inflating: /content/images/anchor_pool/774.png  \n",
            "  inflating: /content/images/anchor_pool/800.png  \n",
            "  inflating: /content/images/anchor_pool/827.png  \n",
            "  inflating: /content/images/anchor_pool/845.png  \n",
            "  inflating: /content/images/anchor_pool/849.png  \n",
            "  inflating: /content/images/anchor_pool/868.png  \n",
            "  inflating: /content/images/anchor_pool/879.png  \n",
            "  inflating: /content/images/anchor_pool/880.png  \n",
            "  inflating: /content/images/anchor_pool/884.png  \n",
            "  inflating: /content/images/anchor_pool/892.png  \n",
            "  inflating: /content/images/anchor_pool/907.png  \n",
            "  inflating: /content/images/anchor_pool/909.png  \n",
            "  inflating: /content/images/anchor_pool/914.png  \n",
            "  inflating: /content/images/anchor_pool/924.png  \n",
            "  inflating: /content/images/anchor_pool/933.png  \n",
            "  inflating: /content/images/anchor_pool/935.png  \n",
            "  inflating: /content/images/anchor_pool/938.png  \n",
            "  inflating: /content/images/anchor_pool/939.png  \n",
            "  inflating: /content/images/anchor_pool/94.png  \n",
            "  inflating: /content/images/anchor_pool/947.png  \n",
            "  inflating: /content/images/anchor_pool/949.png  \n",
            "  inflating: /content/images/anchor_pool/955.png  \n",
            "  inflating: /content/images/anchor_pool/966.png  \n",
            "  inflating: /content/images/anchor_pool/975.png  \n",
            "  inflating: /content/images/anchor_pool/985.png  \n",
            "  inflating: /content/images/anchor_pool/anchor_descriptions.csv  \n",
            "Archive:  /content/drive/MyDrive/images/candidate_images.zip\n",
            "  inflating: /content/images/candidate_images/0.png  \n",
            "  inflating: /content/images/candidate_images/1.png  \n",
            "  inflating: /content/images/candidate_images/10.png  \n",
            "  inflating: /content/images/candidate_images/100.png  \n",
            "  inflating: /content/images/candidate_images/1000.png  \n",
            "  inflating: /content/images/candidate_images/1001.png  \n",
            "  inflating: /content/images/candidate_images/1003.png  \n",
            "  inflating: /content/images/candidate_images/1005.png  \n",
            "  inflating: /content/images/candidate_images/1006.png  \n",
            "  inflating: /content/images/candidate_images/1007.png  \n",
            "  inflating: /content/images/candidate_images/1009.png  \n",
            "  inflating: /content/images/candidate_images/101.png  \n",
            "  inflating: /content/images/candidate_images/1011.png  \n",
            "  inflating: /content/images/candidate_images/1012.png  \n",
            "  inflating: /content/images/candidate_images/1014.png  \n",
            "  inflating: /content/images/candidate_images/1015.png  \n",
            "  inflating: /content/images/candidate_images/1016.png  \n",
            "  inflating: /content/images/candidate_images/1017.png  \n",
            "  inflating: /content/images/candidate_images/1018.png  \n",
            "  inflating: /content/images/candidate_images/1019.png  \n",
            "  inflating: /content/images/candidate_images/102.png  \n",
            "  inflating: /content/images/candidate_images/1020.png  \n",
            "  inflating: /content/images/candidate_images/1021.png  \n",
            "  inflating: /content/images/candidate_images/1022.png  \n",
            "  inflating: /content/images/candidate_images/1023.png  \n",
            "  inflating: /content/images/candidate_images/1024.png  \n",
            "  inflating: /content/images/candidate_images/1025.png  \n",
            "  inflating: /content/images/candidate_images/1026.png  \n",
            "  inflating: /content/images/candidate_images/1027.png  \n",
            "  inflating: /content/images/candidate_images/1028.png  \n",
            "  inflating: /content/images/candidate_images/103.png  \n",
            "  inflating: /content/images/candidate_images/1030.png  \n",
            "  inflating: /content/images/candidate_images/1031.png  \n",
            "  inflating: /content/images/candidate_images/1032.png  \n",
            "  inflating: /content/images/candidate_images/1035.png  \n",
            "  inflating: /content/images/candidate_images/1036.png  \n",
            "  inflating: /content/images/candidate_images/1037.png  \n",
            "  inflating: /content/images/candidate_images/1039.png  \n",
            "  inflating: /content/images/candidate_images/104.png  \n",
            "  inflating: /content/images/candidate_images/1040.png  \n",
            "  inflating: /content/images/candidate_images/1041.png  \n",
            "  inflating: /content/images/candidate_images/1042.png  \n",
            "  inflating: /content/images/candidate_images/1044.png  \n",
            "  inflating: /content/images/candidate_images/1045.png  \n",
            "  inflating: /content/images/candidate_images/1046.png  \n",
            "  inflating: /content/images/candidate_images/1049.png  \n",
            "  inflating: /content/images/candidate_images/1050.png  \n",
            "  inflating: /content/images/candidate_images/1053.png  \n",
            "  inflating: /content/images/candidate_images/1054.png  \n",
            "  inflating: /content/images/candidate_images/1055.png  \n",
            "  inflating: /content/images/candidate_images/1056.png  \n",
            "  inflating: /content/images/candidate_images/106.png  \n",
            "  inflating: /content/images/candidate_images/1060.png  \n",
            "  inflating: /content/images/candidate_images/1061.png  \n",
            "  inflating: /content/images/candidate_images/1062.png  \n",
            "  inflating: /content/images/candidate_images/1063.png  \n",
            "  inflating: /content/images/candidate_images/1064.png  \n",
            "  inflating: /content/images/candidate_images/1066.png  \n",
            "  inflating: /content/images/candidate_images/1067.png  \n",
            "  inflating: /content/images/candidate_images/1068.png  \n",
            "  inflating: /content/images/candidate_images/107.png  \n",
            "  inflating: /content/images/candidate_images/1070.png  \n",
            "  inflating: /content/images/candidate_images/1071.png  \n",
            "  inflating: /content/images/candidate_images/1072.png  \n",
            "  inflating: /content/images/candidate_images/1073.png  \n",
            "  inflating: /content/images/candidate_images/1074.png  \n",
            "  inflating: /content/images/candidate_images/1075.png  \n",
            "  inflating: /content/images/candidate_images/1076.png  \n",
            "  inflating: /content/images/candidate_images/1077.png  \n",
            "  inflating: /content/images/candidate_images/1078.png  \n",
            "  inflating: /content/images/candidate_images/108.png  \n",
            "  inflating: /content/images/candidate_images/1080.png  \n",
            "  inflating: /content/images/candidate_images/1082.png  \n",
            "  inflating: /content/images/candidate_images/1083.png  \n",
            "  inflating: /content/images/candidate_images/1084.png  \n",
            "  inflating: /content/images/candidate_images/1085.png  \n",
            "  inflating: /content/images/candidate_images/1087.png  \n",
            "  inflating: /content/images/candidate_images/1088.png  \n",
            "  inflating: /content/images/candidate_images/1089.png  \n",
            "  inflating: /content/images/candidate_images/109.png  \n",
            "  inflating: /content/images/candidate_images/1090.png  \n",
            "  inflating: /content/images/candidate_images/1091.png  \n",
            "  inflating: /content/images/candidate_images/1093.png  \n",
            "  inflating: /content/images/candidate_images/1094.png  \n",
            "  inflating: /content/images/candidate_images/1098.png  \n",
            "  inflating: /content/images/candidate_images/110.png  \n",
            "  inflating: /content/images/candidate_images/1100.png  \n",
            "  inflating: /content/images/candidate_images/1101.png  \n",
            "  inflating: /content/images/candidate_images/1103.png  \n",
            "  inflating: /content/images/candidate_images/1105.png  \n",
            "  inflating: /content/images/candidate_images/1106.png  \n",
            "  inflating: /content/images/candidate_images/1107.png  \n",
            "  inflating: /content/images/candidate_images/1108.png  \n",
            "  inflating: /content/images/candidate_images/111.png  \n",
            "  inflating: /content/images/candidate_images/1110.png  \n",
            "  inflating: /content/images/candidate_images/1111.png  \n",
            "  inflating: /content/images/candidate_images/1112.png  \n",
            "  inflating: /content/images/candidate_images/1113.png  \n",
            "  inflating: /content/images/candidate_images/1114.png  \n",
            "  inflating: /content/images/candidate_images/1115.png  \n",
            "  inflating: /content/images/candidate_images/1116.png  \n",
            "  inflating: /content/images/candidate_images/1117.png  \n",
            "  inflating: /content/images/candidate_images/1119.png  \n",
            "  inflating: /content/images/candidate_images/112.png  \n",
            "  inflating: /content/images/candidate_images/1120.png  \n",
            "  inflating: /content/images/candidate_images/1121.png  \n",
            "  inflating: /content/images/candidate_images/1123.png  \n",
            "  inflating: /content/images/candidate_images/1124.png  \n",
            "  inflating: /content/images/candidate_images/1125.png  \n",
            "  inflating: /content/images/candidate_images/1126.png  \n",
            "  inflating: /content/images/candidate_images/1127.png  \n",
            "  inflating: /content/images/candidate_images/113.png  \n",
            "  inflating: /content/images/candidate_images/1130.png  \n",
            "  inflating: /content/images/candidate_images/1131.png  \n",
            "  inflating: /content/images/candidate_images/1132.png  \n",
            "  inflating: /content/images/candidate_images/1133.png  \n",
            "  inflating: /content/images/candidate_images/1134.png  \n",
            "  inflating: /content/images/candidate_images/1135.png  \n",
            "  inflating: /content/images/candidate_images/1136.png  \n",
            "  inflating: /content/images/candidate_images/1137.png  \n",
            "  inflating: /content/images/candidate_images/1138.png  \n",
            "  inflating: /content/images/candidate_images/1141.png  \n",
            "  inflating: /content/images/candidate_images/1142.png  \n",
            "  inflating: /content/images/candidate_images/1145.png  \n",
            "  inflating: /content/images/candidate_images/1147.png  \n",
            "  inflating: /content/images/candidate_images/1148.png  \n",
            "  inflating: /content/images/candidate_images/1149.png  \n",
            "  inflating: /content/images/candidate_images/115.png  \n",
            "  inflating: /content/images/candidate_images/1150.png  \n",
            "  inflating: /content/images/candidate_images/1151.png  \n",
            "  inflating: /content/images/candidate_images/1153.png  \n",
            "  inflating: /content/images/candidate_images/1154.png  \n",
            "  inflating: /content/images/candidate_images/1156.png  \n",
            "  inflating: /content/images/candidate_images/1157.png  \n",
            "  inflating: /content/images/candidate_images/1158.png  \n",
            "  inflating: /content/images/candidate_images/116.png  \n",
            "  inflating: /content/images/candidate_images/1160.png  \n",
            "  inflating: /content/images/candidate_images/1161.png  \n",
            "  inflating: /content/images/candidate_images/1163.png  \n",
            "  inflating: /content/images/candidate_images/1164.png  \n",
            "  inflating: /content/images/candidate_images/1167.png  \n",
            "  inflating: /content/images/candidate_images/1169.png  \n",
            "  inflating: /content/images/candidate_images/1170.png  \n",
            "  inflating: /content/images/candidate_images/1171.png  \n",
            "  inflating: /content/images/candidate_images/1172.png  \n",
            "  inflating: /content/images/candidate_images/1175.png  \n",
            "  inflating: /content/images/candidate_images/1176.png  \n",
            "  inflating: /content/images/candidate_images/1177.png  \n",
            "  inflating: /content/images/candidate_images/1178.png  \n",
            "  inflating: /content/images/candidate_images/118.png  \n",
            "  inflating: /content/images/candidate_images/1180.png  \n",
            "  inflating: /content/images/candidate_images/1181.png  \n",
            "  inflating: /content/images/candidate_images/1183.png  \n",
            "  inflating: /content/images/candidate_images/1186.png  \n",
            "  inflating: /content/images/candidate_images/1187.png  \n",
            "  inflating: /content/images/candidate_images/1188.png  \n",
            "  inflating: /content/images/candidate_images/119.png  \n",
            "  inflating: /content/images/candidate_images/1190.png  \n",
            "  inflating: /content/images/candidate_images/1191.png  \n",
            "  inflating: /content/images/candidate_images/1192.png  \n",
            "  inflating: /content/images/candidate_images/1195.png  \n",
            "  inflating: /content/images/candidate_images/1197.png  \n",
            "  inflating: /content/images/candidate_images/1198.png  \n",
            "  inflating: /content/images/candidate_images/1199.png  \n",
            "  inflating: /content/images/candidate_images/12.png  \n",
            "  inflating: /content/images/candidate_images/120.png  \n",
            "  inflating: /content/images/candidate_images/1201.png  \n",
            "  inflating: /content/images/candidate_images/1203.png  \n",
            "  inflating: /content/images/candidate_images/1204.png  \n",
            "  inflating: /content/images/candidate_images/1205.png  \n",
            "  inflating: /content/images/candidate_images/1207.png  \n",
            "  inflating: /content/images/candidate_images/1208.png  \n",
            "  inflating: /content/images/candidate_images/1209.png  \n",
            "  inflating: /content/images/candidate_images/121.png  \n",
            "  inflating: /content/images/candidate_images/1210.png  \n",
            "  inflating: /content/images/candidate_images/1211.png  \n",
            "  inflating: /content/images/candidate_images/1213.png  \n",
            "  inflating: /content/images/candidate_images/1214.png  \n",
            "  inflating: /content/images/candidate_images/1215.png  \n",
            "  inflating: /content/images/candidate_images/1217.png  \n",
            "  inflating: /content/images/candidate_images/1219.png  \n",
            "  inflating: /content/images/candidate_images/122.png  \n",
            "  inflating: /content/images/candidate_images/1221.png  \n",
            "  inflating: /content/images/candidate_images/1222.png  \n",
            "  inflating: /content/images/candidate_images/1223.png  \n",
            "  inflating: /content/images/candidate_images/1224.png  \n",
            "  inflating: /content/images/candidate_images/1225.png  \n",
            "  inflating: /content/images/candidate_images/1226.png  \n",
            "  inflating: /content/images/candidate_images/1227.png  \n",
            "  inflating: /content/images/candidate_images/123.png  \n",
            "  inflating: /content/images/candidate_images/1230.png  \n",
            "  inflating: /content/images/candidate_images/1231.png  \n",
            "  inflating: /content/images/candidate_images/1232.png  \n",
            "  inflating: /content/images/candidate_images/1233.png  \n",
            "  inflating: /content/images/candidate_images/1234.png  \n",
            "  inflating: /content/images/candidate_images/1236.png  \n",
            "  inflating: /content/images/candidate_images/1238.png  \n",
            "  inflating: /content/images/candidate_images/1239.png  \n",
            "  inflating: /content/images/candidate_images/124.png  \n",
            "  inflating: /content/images/candidate_images/1240.png  \n",
            "  inflating: /content/images/candidate_images/1242.png  \n",
            "  inflating: /content/images/candidate_images/1243.png  \n",
            "  inflating: /content/images/candidate_images/1244.png  \n",
            "  inflating: /content/images/candidate_images/1245.png  \n",
            "  inflating: /content/images/candidate_images/1246.png  \n",
            "  inflating: /content/images/candidate_images/1247.png  \n",
            "  inflating: /content/images/candidate_images/1248.png  \n",
            "  inflating: /content/images/candidate_images/1249.png  \n",
            "  inflating: /content/images/candidate_images/125.png  \n",
            "  inflating: /content/images/candidate_images/1253.png  \n",
            "  inflating: /content/images/candidate_images/1254.png  \n",
            "  inflating: /content/images/candidate_images/1255.png  \n",
            "  inflating: /content/images/candidate_images/1256.png  \n",
            "  inflating: /content/images/candidate_images/1258.png  \n",
            "  inflating: /content/images/candidate_images/1259.png  \n",
            "  inflating: /content/images/candidate_images/126.png  \n",
            "  inflating: /content/images/candidate_images/1260.png  \n",
            "  inflating: /content/images/candidate_images/1264.png  \n",
            "  inflating: /content/images/candidate_images/1265.png  \n",
            "  inflating: /content/images/candidate_images/1266.png  \n",
            "  inflating: /content/images/candidate_images/1268.png  \n",
            "  inflating: /content/images/candidate_images/1269.png  \n",
            "  inflating: /content/images/candidate_images/1270.png  \n",
            "  inflating: /content/images/candidate_images/1272.png  \n",
            "  inflating: /content/images/candidate_images/1273.png  \n",
            "  inflating: /content/images/candidate_images/1275.png  \n",
            "  inflating: /content/images/candidate_images/1278.png  \n",
            "  inflating: /content/images/candidate_images/1279.png  \n",
            "  inflating: /content/images/candidate_images/128.png  \n",
            "  inflating: /content/images/candidate_images/1280.png  \n",
            "  inflating: /content/images/candidate_images/1282.png  \n",
            "  inflating: /content/images/candidate_images/1283.png  \n",
            "  inflating: /content/images/candidate_images/1284.png  \n",
            "  inflating: /content/images/candidate_images/1285.png  \n",
            "  inflating: /content/images/candidate_images/1286.png  \n",
            "  inflating: /content/images/candidate_images/1287.png  \n",
            "  inflating: /content/images/candidate_images/1289.png  \n",
            "  inflating: /content/images/candidate_images/1291.png  \n",
            "  inflating: /content/images/candidate_images/1292.png  \n",
            "  inflating: /content/images/candidate_images/1293.png  \n",
            "  inflating: /content/images/candidate_images/1295.png  \n",
            "  inflating: /content/images/candidate_images/1296.png  \n",
            "  inflating: /content/images/candidate_images/13.png  \n",
            "  inflating: /content/images/candidate_images/130.png  \n",
            "  inflating: /content/images/candidate_images/1300.png  \n",
            "  inflating: /content/images/candidate_images/1301.png  \n",
            "  inflating: /content/images/candidate_images/1302.png  \n",
            "  inflating: /content/images/candidate_images/1303.png  \n",
            "  inflating: /content/images/candidate_images/1305.png  \n",
            "  inflating: /content/images/candidate_images/1306.png  \n",
            "  inflating: /content/images/candidate_images/1307.png  \n",
            "  inflating: /content/images/candidate_images/1309.png  \n",
            "  inflating: /content/images/candidate_images/131.png  \n",
            "  inflating: /content/images/candidate_images/1311.png  \n",
            "  inflating: /content/images/candidate_images/1312.png  \n",
            "  inflating: /content/images/candidate_images/1314.png  \n",
            "  inflating: /content/images/candidate_images/1315.png  \n",
            "  inflating: /content/images/candidate_images/1316.png  \n",
            "  inflating: /content/images/candidate_images/1317.png  \n",
            "  inflating: /content/images/candidate_images/1318.png  \n",
            "  inflating: /content/images/candidate_images/1319.png  \n",
            "  inflating: /content/images/candidate_images/132.png  \n",
            "  inflating: /content/images/candidate_images/1321.png  \n",
            "  inflating: /content/images/candidate_images/1322.png  \n",
            "  inflating: /content/images/candidate_images/1324.png  \n",
            "  inflating: /content/images/candidate_images/1326.png  \n",
            "  inflating: /content/images/candidate_images/1329.png  \n",
            "  inflating: /content/images/candidate_images/133.png  \n",
            "  inflating: /content/images/candidate_images/1335.png  \n",
            "  inflating: /content/images/candidate_images/1336.png  \n",
            "  inflating: /content/images/candidate_images/1337.png  \n",
            "  inflating: /content/images/candidate_images/1339.png  \n",
            "  inflating: /content/images/candidate_images/134.png  \n",
            "  inflating: /content/images/candidate_images/1341.png  \n",
            "  inflating: /content/images/candidate_images/1342.png  \n",
            "  inflating: /content/images/candidate_images/1346.png  \n",
            "  inflating: /content/images/candidate_images/1347.png  \n",
            "  inflating: /content/images/candidate_images/1348.png  \n",
            "  inflating: /content/images/candidate_images/1349.png  \n",
            "  inflating: /content/images/candidate_images/1350.png  \n",
            "  inflating: /content/images/candidate_images/1353.png  \n",
            "  inflating: /content/images/candidate_images/1356.png  \n",
            "  inflating: /content/images/candidate_images/1357.png  \n",
            "  inflating: /content/images/candidate_images/1359.png  \n",
            "  inflating: /content/images/candidate_images/136.png  \n",
            "  inflating: /content/images/candidate_images/1360.png  \n",
            "  inflating: /content/images/candidate_images/1361.png  \n",
            "  inflating: /content/images/candidate_images/1362.png  \n",
            "  inflating: /content/images/candidate_images/1363.png  \n",
            "  inflating: /content/images/candidate_images/1366.png  \n",
            "  inflating: /content/images/candidate_images/137.png  \n",
            "  inflating: /content/images/candidate_images/1370.png  \n",
            "  inflating: /content/images/candidate_images/1371.png  \n",
            "  inflating: /content/images/candidate_images/1374.png  \n",
            "  inflating: /content/images/candidate_images/1375.png  \n",
            "  inflating: /content/images/candidate_images/1376.png  \n",
            "  inflating: /content/images/candidate_images/1377.png  \n",
            "  inflating: /content/images/candidate_images/1379.png  \n",
            "  inflating: /content/images/candidate_images/138.png  \n",
            "  inflating: /content/images/candidate_images/1380.png  \n",
            "  inflating: /content/images/candidate_images/1381.png  \n",
            "  inflating: /content/images/candidate_images/1382.png  \n",
            "  inflating: /content/images/candidate_images/1383.png  \n",
            "  inflating: /content/images/candidate_images/1385.png  \n",
            "  inflating: /content/images/candidate_images/1387.png  \n",
            "  inflating: /content/images/candidate_images/1388.png  \n",
            "  inflating: /content/images/candidate_images/1389.png  \n",
            "  inflating: /content/images/candidate_images/139.png  \n",
            "  inflating: /content/images/candidate_images/1390.png  \n",
            "  inflating: /content/images/candidate_images/1391.png  \n",
            "  inflating: /content/images/candidate_images/1392.png  \n",
            "  inflating: /content/images/candidate_images/1394.png  \n",
            "  inflating: /content/images/candidate_images/1396.png  \n",
            "  inflating: /content/images/candidate_images/1399.png  \n",
            "  inflating: /content/images/candidate_images/1400.png  \n",
            "  inflating: /content/images/candidate_images/1401.png  \n",
            "  inflating: /content/images/candidate_images/1402.png  \n",
            "  inflating: /content/images/candidate_images/1403.png  \n",
            "  inflating: /content/images/candidate_images/1405.png  \n",
            "  inflating: /content/images/candidate_images/1406.png  \n",
            "  inflating: /content/images/candidate_images/1408.png  \n",
            "  inflating: /content/images/candidate_images/141.png  \n",
            "  inflating: /content/images/candidate_images/1410.png  \n",
            "  inflating: /content/images/candidate_images/1411.png  \n",
            "  inflating: /content/images/candidate_images/1413.png  \n",
            "  inflating: /content/images/candidate_images/1414.png  \n",
            "  inflating: /content/images/candidate_images/1416.png  \n",
            "  inflating: /content/images/candidate_images/1417.png  \n",
            "  inflating: /content/images/candidate_images/1419.png  \n",
            "  inflating: /content/images/candidate_images/1420.png  \n",
            "  inflating: /content/images/candidate_images/1421.png  \n",
            "  inflating: /content/images/candidate_images/1425.png  \n",
            "  inflating: /content/images/candidate_images/1426.png  \n",
            "  inflating: /content/images/candidate_images/1427.png  \n",
            "  inflating: /content/images/candidate_images/1428.png  \n",
            "  inflating: /content/images/candidate_images/1429.png  \n",
            "  inflating: /content/images/candidate_images/143.png  \n",
            "  inflating: /content/images/candidate_images/1430.png  \n",
            "  inflating: /content/images/candidate_images/1431.png  \n",
            "  inflating: /content/images/candidate_images/1432.png  \n",
            "  inflating: /content/images/candidate_images/1434.png  \n",
            "  inflating: /content/images/candidate_images/1436.png  \n",
            "  inflating: /content/images/candidate_images/1437.png  \n",
            "  inflating: /content/images/candidate_images/144.png  \n",
            "  inflating: /content/images/candidate_images/1440.png  \n",
            "  inflating: /content/images/candidate_images/1441.png  \n",
            "  inflating: /content/images/candidate_images/1442.png  \n",
            "  inflating: /content/images/candidate_images/1447.png  \n",
            "  inflating: /content/images/candidate_images/1451.png  \n",
            "  inflating: /content/images/candidate_images/1456.png  \n",
            "  inflating: /content/images/candidate_images/1461.png  \n",
            "  inflating: /content/images/candidate_images/1462.png  \n",
            "  inflating: /content/images/candidate_images/1464.png  \n",
            "  inflating: /content/images/candidate_images/1466.png  \n",
            "  inflating: /content/images/candidate_images/147.png  \n",
            "  inflating: /content/images/candidate_images/1470.png  \n",
            "  inflating: /content/images/candidate_images/1472.png  \n",
            "  inflating: /content/images/candidate_images/1473.png  \n",
            "  inflating: /content/images/candidate_images/1474.png  \n",
            "  inflating: /content/images/candidate_images/1475.png  \n",
            "  inflating: /content/images/candidate_images/1477.png  \n",
            "  inflating: /content/images/candidate_images/1478.png  \n",
            "  inflating: /content/images/candidate_images/1480.png  \n",
            "  inflating: /content/images/candidate_images/1482.png  \n",
            "  inflating: /content/images/candidate_images/1483.png  \n",
            "  inflating: /content/images/candidate_images/1485.png  \n",
            "  inflating: /content/images/candidate_images/1486.png  \n",
            "  inflating: /content/images/candidate_images/1487.png  \n",
            "  inflating: /content/images/candidate_images/1488.png  \n",
            "  inflating: /content/images/candidate_images/149.png  \n",
            "  inflating: /content/images/candidate_images/1490.png  \n",
            "  inflating: /content/images/candidate_images/1493.png  \n",
            "  inflating: /content/images/candidate_images/1494.png  \n",
            "  inflating: /content/images/candidate_images/1495.png  \n",
            "  inflating: /content/images/candidate_images/1496.png  \n",
            "  inflating: /content/images/candidate_images/1497.png  \n",
            "  inflating: /content/images/candidate_images/1498.png  \n",
            "  inflating: /content/images/candidate_images/1499.png  \n",
            "  inflating: /content/images/candidate_images/15.png  \n",
            "  inflating: /content/images/candidate_images/150.png  \n",
            "  inflating: /content/images/candidate_images/1501.png  \n",
            "  inflating: /content/images/candidate_images/1502.png  \n",
            "  inflating: /content/images/candidate_images/1504.png  \n",
            "  inflating: /content/images/candidate_images/1505.png  \n",
            "  inflating: /content/images/candidate_images/1506.png  \n",
            "  inflating: /content/images/candidate_images/1508.png  \n",
            "  inflating: /content/images/candidate_images/151.png  \n",
            "  inflating: /content/images/candidate_images/1511.png  \n",
            "  inflating: /content/images/candidate_images/1512.png  \n",
            "  inflating: /content/images/candidate_images/1513.png  \n",
            "  inflating: /content/images/candidate_images/1514.png  \n",
            "  inflating: /content/images/candidate_images/1515.png  \n",
            "  inflating: /content/images/candidate_images/1516.png  \n",
            "  inflating: /content/images/candidate_images/152.png  \n",
            "  inflating: /content/images/candidate_images/153.png  \n",
            "  inflating: /content/images/candidate_images/155.png  \n",
            "  inflating: /content/images/candidate_images/156.png  \n",
            "  inflating: /content/images/candidate_images/157.png  \n",
            "  inflating: /content/images/candidate_images/159.png  \n",
            "  inflating: /content/images/candidate_images/16.png  \n",
            "  inflating: /content/images/candidate_images/160.png  \n",
            "  inflating: /content/images/candidate_images/161.png  \n",
            "  inflating: /content/images/candidate_images/162.png  \n",
            "  inflating: /content/images/candidate_images/163.png  \n",
            "  inflating: /content/images/candidate_images/164.png  \n",
            "  inflating: /content/images/candidate_images/165.png  \n",
            "  inflating: /content/images/candidate_images/166.png  \n",
            "  inflating: /content/images/candidate_images/167.png  \n",
            "  inflating: /content/images/candidate_images/169.png  \n",
            "  inflating: /content/images/candidate_images/17.png  \n",
            "  inflating: /content/images/candidate_images/170.png  \n",
            "  inflating: /content/images/candidate_images/173.png  \n",
            "  inflating: /content/images/candidate_images/174.png  \n",
            "  inflating: /content/images/candidate_images/176.png  \n",
            "  inflating: /content/images/candidate_images/177.png  \n",
            "  inflating: /content/images/candidate_images/178.png  \n",
            "  inflating: /content/images/candidate_images/18.png  \n",
            "  inflating: /content/images/candidate_images/180.png  \n",
            "  inflating: /content/images/candidate_images/182.png  \n",
            "  inflating: /content/images/candidate_images/184.png  \n",
            "  inflating: /content/images/candidate_images/185.png  \n",
            "  inflating: /content/images/candidate_images/186.png  \n",
            "  inflating: /content/images/candidate_images/187.png  \n",
            "  inflating: /content/images/candidate_images/188.png  \n",
            "  inflating: /content/images/candidate_images/19.png  \n",
            "  inflating: /content/images/candidate_images/190.png  \n",
            "  inflating: /content/images/candidate_images/191.png  \n",
            "  inflating: /content/images/candidate_images/192.png  \n",
            "  inflating: /content/images/candidate_images/193.png  \n",
            "  inflating: /content/images/candidate_images/194.png  \n",
            "  inflating: /content/images/candidate_images/195.png  \n",
            "  inflating: /content/images/candidate_images/196.png  \n",
            "  inflating: /content/images/candidate_images/197.png  \n",
            "  inflating: /content/images/candidate_images/198.png  \n",
            "  inflating: /content/images/candidate_images/199.png  \n",
            "  inflating: /content/images/candidate_images/2.png  \n",
            "  inflating: /content/images/candidate_images/20.png  \n",
            "  inflating: /content/images/candidate_images/202.png  \n",
            "  inflating: /content/images/candidate_images/203.png  \n",
            "  inflating: /content/images/candidate_images/204.png  \n",
            "  inflating: /content/images/candidate_images/205.png  \n",
            "  inflating: /content/images/candidate_images/206.png  \n",
            "  inflating: /content/images/candidate_images/207.png  \n",
            "  inflating: /content/images/candidate_images/208.png  \n",
            "  inflating: /content/images/candidate_images/21.png  \n",
            "  inflating: /content/images/candidate_images/210.png  \n",
            "  inflating: /content/images/candidate_images/211.png  \n",
            "  inflating: /content/images/candidate_images/213.png  \n",
            "  inflating: /content/images/candidate_images/216.png  \n",
            "  inflating: /content/images/candidate_images/217.png  \n",
            "  inflating: /content/images/candidate_images/219.png  \n",
            "  inflating: /content/images/candidate_images/221.png  \n",
            "  inflating: /content/images/candidate_images/222.png  \n",
            "  inflating: /content/images/candidate_images/223.png  \n",
            "  inflating: /content/images/candidate_images/225.png  \n",
            "  inflating: /content/images/candidate_images/226.png  \n",
            "  inflating: /content/images/candidate_images/227.png  \n",
            "  inflating: /content/images/candidate_images/229.png  \n",
            "  inflating: /content/images/candidate_images/23.png  \n",
            "  inflating: /content/images/candidate_images/230.png  \n",
            "  inflating: /content/images/candidate_images/231.png  \n",
            "  inflating: /content/images/candidate_images/232.png  \n",
            "  inflating: /content/images/candidate_images/234.png  \n",
            "  inflating: /content/images/candidate_images/235.png  \n",
            "  inflating: /content/images/candidate_images/236.png  \n",
            "  inflating: /content/images/candidate_images/237.png  \n",
            "  inflating: /content/images/candidate_images/238.png  \n",
            "  inflating: /content/images/candidate_images/239.png  \n",
            "  inflating: /content/images/candidate_images/24.png  \n",
            "  inflating: /content/images/candidate_images/240.png  \n",
            "  inflating: /content/images/candidate_images/241.png  \n",
            "  inflating: /content/images/candidate_images/242.png  \n",
            "  inflating: /content/images/candidate_images/243.png  \n",
            "  inflating: /content/images/candidate_images/244.png  \n",
            "  inflating: /content/images/candidate_images/245.png  \n",
            "  inflating: /content/images/candidate_images/247.png  \n",
            "  inflating: /content/images/candidate_images/248.png  \n",
            "  inflating: /content/images/candidate_images/25.png  \n",
            "  inflating: /content/images/candidate_images/251.png  \n",
            "  inflating: /content/images/candidate_images/252.png  \n",
            "  inflating: /content/images/candidate_images/253.png  \n",
            "  inflating: /content/images/candidate_images/254.png  \n",
            "  inflating: /content/images/candidate_images/255.png  \n",
            "  inflating: /content/images/candidate_images/256.png  \n",
            "  inflating: /content/images/candidate_images/257.png  \n",
            "  inflating: /content/images/candidate_images/258.png  \n",
            "  inflating: /content/images/candidate_images/26.png  \n",
            "  inflating: /content/images/candidate_images/260.png  \n",
            "  inflating: /content/images/candidate_images/261.png  \n",
            "  inflating: /content/images/candidate_images/262.png  \n",
            "  inflating: /content/images/candidate_images/263.png  \n",
            "  inflating: /content/images/candidate_images/264.png  \n",
            "  inflating: /content/images/candidate_images/265.png  \n",
            "  inflating: /content/images/candidate_images/266.png  \n",
            "  inflating: /content/images/candidate_images/267.png  \n",
            "  inflating: /content/images/candidate_images/270.png  \n",
            "  inflating: /content/images/candidate_images/272.png  \n",
            "  inflating: /content/images/candidate_images/273.png  \n",
            "  inflating: /content/images/candidate_images/274.png  \n",
            "  inflating: /content/images/candidate_images/275.png  \n",
            "  inflating: /content/images/candidate_images/277.png  \n",
            "  inflating: /content/images/candidate_images/279.png  \n",
            "  inflating: /content/images/candidate_images/280.png  \n",
            "  inflating: /content/images/candidate_images/281.png  \n",
            "  inflating: /content/images/candidate_images/282.png  \n",
            "  inflating: /content/images/candidate_images/283.png  \n",
            "  inflating: /content/images/candidate_images/284.png  \n",
            "  inflating: /content/images/candidate_images/286.png  \n",
            "  inflating: /content/images/candidate_images/287.png  \n",
            "  inflating: /content/images/candidate_images/288.png  \n",
            "  inflating: /content/images/candidate_images/289.png  \n",
            "  inflating: /content/images/candidate_images/29.png  \n",
            "  inflating: /content/images/candidate_images/290.png  \n",
            "  inflating: /content/images/candidate_images/291.png  \n",
            "  inflating: /content/images/candidate_images/292.png  \n",
            "  inflating: /content/images/candidate_images/293.png  \n",
            "  inflating: /content/images/candidate_images/294.png  \n",
            "  inflating: /content/images/candidate_images/295.png  \n",
            "  inflating: /content/images/candidate_images/296.png  \n",
            "  inflating: /content/images/candidate_images/297.png  \n",
            "  inflating: /content/images/candidate_images/298.png  \n",
            "  inflating: /content/images/candidate_images/299.png  \n",
            "  inflating: /content/images/candidate_images/3.png  \n",
            "  inflating: /content/images/candidate_images/30.png  \n",
            "  inflating: /content/images/candidate_images/301.png  \n",
            "  inflating: /content/images/candidate_images/302.png  \n",
            "  inflating: /content/images/candidate_images/303.png  \n",
            "  inflating: /content/images/candidate_images/304.png  \n",
            "  inflating: /content/images/candidate_images/306.png  \n",
            "  inflating: /content/images/candidate_images/307.png  \n",
            "  inflating: /content/images/candidate_images/308.png  \n",
            "  inflating: /content/images/candidate_images/309.png  \n",
            "  inflating: /content/images/candidate_images/31.png  \n",
            "  inflating: /content/images/candidate_images/310.png  \n",
            "  inflating: /content/images/candidate_images/311.png  \n",
            "  inflating: /content/images/candidate_images/312.png  \n",
            "  inflating: /content/images/candidate_images/314.png  \n",
            "  inflating: /content/images/candidate_images/315.png  \n",
            "  inflating: /content/images/candidate_images/317.png  \n",
            "  inflating: /content/images/candidate_images/319.png  \n",
            "  inflating: /content/images/candidate_images/32.png  \n",
            "  inflating: /content/images/candidate_images/320.png  \n",
            "  inflating: /content/images/candidate_images/321.png  \n",
            "  inflating: /content/images/candidate_images/322.png  \n",
            "  inflating: /content/images/candidate_images/323.png  \n",
            "  inflating: /content/images/candidate_images/324.png  \n",
            "  inflating: /content/images/candidate_images/325.png  \n",
            "  inflating: /content/images/candidate_images/326.png  \n",
            "  inflating: /content/images/candidate_images/328.png  \n",
            "  inflating: /content/images/candidate_images/33.png  \n",
            "  inflating: /content/images/candidate_images/330.png  \n",
            "  inflating: /content/images/candidate_images/331.png  \n",
            "  inflating: /content/images/candidate_images/333.png  \n",
            "  inflating: /content/images/candidate_images/334.png  \n",
            "  inflating: /content/images/candidate_images/335.png  \n",
            "  inflating: /content/images/candidate_images/336.png  \n",
            "  inflating: /content/images/candidate_images/337.png  \n",
            "  inflating: /content/images/candidate_images/338.png  \n",
            "  inflating: /content/images/candidate_images/339.png  \n",
            "  inflating: /content/images/candidate_images/34.png  \n",
            "  inflating: /content/images/candidate_images/340.png  \n",
            "  inflating: /content/images/candidate_images/341.png  \n",
            "  inflating: /content/images/candidate_images/342.png  \n",
            "  inflating: /content/images/candidate_images/343.png  \n",
            "  inflating: /content/images/candidate_images/344.png  \n",
            "  inflating: /content/images/candidate_images/346.png  \n",
            "  inflating: /content/images/candidate_images/347.png  \n",
            "  inflating: /content/images/candidate_images/348.png  \n",
            "  inflating: /content/images/candidate_images/35.png  \n",
            "  inflating: /content/images/candidate_images/350.png  \n",
            "  inflating: /content/images/candidate_images/352.png  \n",
            "  inflating: /content/images/candidate_images/354.png  \n",
            "  inflating: /content/images/candidate_images/355.png  \n",
            "  inflating: /content/images/candidate_images/356.png  \n",
            "  inflating: /content/images/candidate_images/358.png  \n",
            "  inflating: /content/images/candidate_images/359.png  \n",
            "  inflating: /content/images/candidate_images/36.png  \n",
            "  inflating: /content/images/candidate_images/360.png  \n",
            "  inflating: /content/images/candidate_images/361.png  \n",
            "  inflating: /content/images/candidate_images/363.png  \n",
            "  inflating: /content/images/candidate_images/366.png  \n",
            "  inflating: /content/images/candidate_images/367.png  \n",
            "  inflating: /content/images/candidate_images/368.png  \n",
            "  inflating: /content/images/candidate_images/370.png  \n",
            "  inflating: /content/images/candidate_images/371.png  \n",
            "  inflating: /content/images/candidate_images/372.png  \n",
            "  inflating: /content/images/candidate_images/373.png  \n",
            "  inflating: /content/images/candidate_images/374.png  \n",
            "  inflating: /content/images/candidate_images/375.png  \n",
            "  inflating: /content/images/candidate_images/376.png  \n",
            "  inflating: /content/images/candidate_images/377.png  \n",
            "  inflating: /content/images/candidate_images/378.png  \n",
            "  inflating: /content/images/candidate_images/379.png  \n",
            "  inflating: /content/images/candidate_images/38.png  \n",
            "  inflating: /content/images/candidate_images/380.png  \n",
            "  inflating: /content/images/candidate_images/381.png  \n",
            "  inflating: /content/images/candidate_images/383.png  \n",
            "  inflating: /content/images/candidate_images/384.png  \n",
            "  inflating: /content/images/candidate_images/387.png  \n",
            "  inflating: /content/images/candidate_images/388.png  \n",
            "  inflating: /content/images/candidate_images/389.png  \n",
            "  inflating: /content/images/candidate_images/39.png  \n",
            "  inflating: /content/images/candidate_images/390.png  \n",
            "  inflating: /content/images/candidate_images/391.png  \n",
            "  inflating: /content/images/candidate_images/392.png  \n",
            "  inflating: /content/images/candidate_images/393.png  \n",
            "  inflating: /content/images/candidate_images/394.png  \n",
            "  inflating: /content/images/candidate_images/395.png  \n",
            "  inflating: /content/images/candidate_images/397.png  \n",
            "  inflating: /content/images/candidate_images/398.png  \n",
            "  inflating: /content/images/candidate_images/399.png  \n",
            "  inflating: /content/images/candidate_images/4.png  \n",
            "  inflating: /content/images/candidate_images/40.png  \n",
            "  inflating: /content/images/candidate_images/401.png  \n",
            "  inflating: /content/images/candidate_images/403.png  \n",
            "  inflating: /content/images/candidate_images/405.png  \n",
            "  inflating: /content/images/candidate_images/406.png  \n",
            "  inflating: /content/images/candidate_images/409.png  \n",
            "  inflating: /content/images/candidate_images/41.png  \n",
            "  inflating: /content/images/candidate_images/410.png  \n",
            "  inflating: /content/images/candidate_images/411.png  \n",
            "  inflating: /content/images/candidate_images/414.png  \n",
            "  inflating: /content/images/candidate_images/416.png  \n",
            "  inflating: /content/images/candidate_images/417.png  \n",
            "  inflating: /content/images/candidate_images/418.png  \n",
            "  inflating: /content/images/candidate_images/419.png  \n",
            "  inflating: /content/images/candidate_images/42.png  \n",
            "  inflating: /content/images/candidate_images/421.png  \n",
            "  inflating: /content/images/candidate_images/422.png  \n",
            "  inflating: /content/images/candidate_images/423.png  \n",
            "  inflating: /content/images/candidate_images/424.png  \n",
            "  inflating: /content/images/candidate_images/427.png  \n",
            "  inflating: /content/images/candidate_images/428.png  \n",
            "  inflating: /content/images/candidate_images/429.png  \n",
            "  inflating: /content/images/candidate_images/43.png  \n",
            "  inflating: /content/images/candidate_images/432.png  \n",
            "  inflating: /content/images/candidate_images/433.png  \n",
            "  inflating: /content/images/candidate_images/434.png  \n",
            "  inflating: /content/images/candidate_images/435.png  \n",
            "  inflating: /content/images/candidate_images/436.png  \n",
            "  inflating: /content/images/candidate_images/437.png  \n",
            "  inflating: /content/images/candidate_images/438.png  \n",
            "  inflating: /content/images/candidate_images/439.png  \n",
            "  inflating: /content/images/candidate_images/44.png  \n",
            "  inflating: /content/images/candidate_images/440.png  \n",
            "  inflating: /content/images/candidate_images/441.png  \n",
            "  inflating: /content/images/candidate_images/443.png  \n",
            "  inflating: /content/images/candidate_images/444.png  \n",
            "  inflating: /content/images/candidate_images/446.png  \n",
            "  inflating: /content/images/candidate_images/447.png  \n",
            "  inflating: /content/images/candidate_images/448.png  \n",
            "  inflating: /content/images/candidate_images/45.png  \n",
            "  inflating: /content/images/candidate_images/450.png  \n",
            "  inflating: /content/images/candidate_images/451.png  \n",
            "  inflating: /content/images/candidate_images/452.png  \n",
            "  inflating: /content/images/candidate_images/453.png  \n",
            "  inflating: /content/images/candidate_images/454.png  \n",
            "  inflating: /content/images/candidate_images/455.png  \n",
            "  inflating: /content/images/candidate_images/457.png  \n",
            "  inflating: /content/images/candidate_images/458.png  \n",
            "  inflating: /content/images/candidate_images/459.png  \n",
            "  inflating: /content/images/candidate_images/46.png  \n",
            "  inflating: /content/images/candidate_images/460.png  \n",
            "  inflating: /content/images/candidate_images/461.png  \n",
            "  inflating: /content/images/candidate_images/462.png  \n",
            "  inflating: /content/images/candidate_images/464.png  \n",
            "  inflating: /content/images/candidate_images/465.png  \n",
            "  inflating: /content/images/candidate_images/466.png  \n",
            "  inflating: /content/images/candidate_images/467.png  \n",
            "  inflating: /content/images/candidate_images/468.png  \n",
            "  inflating: /content/images/candidate_images/469.png  \n",
            "  inflating: /content/images/candidate_images/470.png  \n",
            "  inflating: /content/images/candidate_images/471.png  \n",
            "  inflating: /content/images/candidate_images/472.png  \n",
            "  inflating: /content/images/candidate_images/473.png  \n",
            "  inflating: /content/images/candidate_images/474.png  \n",
            "  inflating: /content/images/candidate_images/475.png  \n",
            "  inflating: /content/images/candidate_images/476.png  \n",
            "  inflating: /content/images/candidate_images/477.png  \n",
            "  inflating: /content/images/candidate_images/478.png  \n",
            "  inflating: /content/images/candidate_images/479.png  \n",
            "  inflating: /content/images/candidate_images/48.png  \n",
            "  inflating: /content/images/candidate_images/480.png  \n",
            "  inflating: /content/images/candidate_images/481.png  \n",
            "  inflating: /content/images/candidate_images/483.png  \n",
            "  inflating: /content/images/candidate_images/484.png  \n",
            "  inflating: /content/images/candidate_images/485.png  \n",
            "  inflating: /content/images/candidate_images/488.png  \n",
            "  inflating: /content/images/candidate_images/489.png  \n",
            "  inflating: /content/images/candidate_images/49.png  \n",
            "  inflating: /content/images/candidate_images/493.png  \n",
            "  inflating: /content/images/candidate_images/494.png  \n",
            "  inflating: /content/images/candidate_images/495.png  \n",
            "  inflating: /content/images/candidate_images/496.png  \n",
            "  inflating: /content/images/candidate_images/498.png  \n",
            "  inflating: /content/images/candidate_images/499.png  \n",
            "  inflating: /content/images/candidate_images/5.png  \n",
            "  inflating: /content/images/candidate_images/501.png  \n",
            "  inflating: /content/images/candidate_images/502.png  \n",
            "  inflating: /content/images/candidate_images/505.png  \n",
            "  inflating: /content/images/candidate_images/506.png  \n",
            "  inflating: /content/images/candidate_images/507.png  \n",
            "  inflating: /content/images/candidate_images/508.png  \n",
            "  inflating: /content/images/candidate_images/509.png  \n",
            "  inflating: /content/images/candidate_images/51.png  \n",
            "  inflating: /content/images/candidate_images/510.png  \n",
            "  inflating: /content/images/candidate_images/511.png  \n",
            "  inflating: /content/images/candidate_images/512.png  \n",
            "  inflating: /content/images/candidate_images/513.png  \n",
            "  inflating: /content/images/candidate_images/515.png  \n",
            "  inflating: /content/images/candidate_images/516.png  \n",
            "  inflating: /content/images/candidate_images/517.png  \n",
            "  inflating: /content/images/candidate_images/519.png  \n",
            "  inflating: /content/images/candidate_images/52.png  \n",
            "  inflating: /content/images/candidate_images/520.png  \n",
            "  inflating: /content/images/candidate_images/521.png  \n",
            "  inflating: /content/images/candidate_images/522.png  \n",
            "  inflating: /content/images/candidate_images/524.png  \n",
            "  inflating: /content/images/candidate_images/527.png  \n",
            "  inflating: /content/images/candidate_images/528.png  \n",
            "  inflating: /content/images/candidate_images/529.png  \n",
            "  inflating: /content/images/candidate_images/53.png  \n",
            "  inflating: /content/images/candidate_images/530.png  \n",
            "  inflating: /content/images/candidate_images/531.png  \n",
            "  inflating: /content/images/candidate_images/532.png  \n",
            "  inflating: /content/images/candidate_images/533.png  \n",
            "  inflating: /content/images/candidate_images/535.png  \n",
            "  inflating: /content/images/candidate_images/536.png  \n",
            "  inflating: /content/images/candidate_images/537.png  \n",
            "  inflating: /content/images/candidate_images/539.png  \n",
            "  inflating: /content/images/candidate_images/540.png  \n",
            "  inflating: /content/images/candidate_images/542.png  \n",
            "  inflating: /content/images/candidate_images/543.png  \n",
            "  inflating: /content/images/candidate_images/545.png  \n",
            "  inflating: /content/images/candidate_images/546.png  \n",
            "  inflating: /content/images/candidate_images/547.png  \n",
            "  inflating: /content/images/candidate_images/548.png  \n",
            "  inflating: /content/images/candidate_images/549.png  \n",
            "  inflating: /content/images/candidate_images/55.png  \n",
            "  inflating: /content/images/candidate_images/550.png  \n",
            "  inflating: /content/images/candidate_images/551.png  \n",
            "  inflating: /content/images/candidate_images/553.png  \n",
            "  inflating: /content/images/candidate_images/554.png  \n",
            "  inflating: /content/images/candidate_images/555.png  \n",
            "  inflating: /content/images/candidate_images/557.png  \n",
            "  inflating: /content/images/candidate_images/558.png  \n",
            "  inflating: /content/images/candidate_images/559.png  \n",
            "  inflating: /content/images/candidate_images/56.png  \n",
            "  inflating: /content/images/candidate_images/560.png  \n",
            "  inflating: /content/images/candidate_images/562.png  \n",
            "  inflating: /content/images/candidate_images/563.png  \n",
            "  inflating: /content/images/candidate_images/564.png  \n",
            "  inflating: /content/images/candidate_images/565.png  \n",
            "  inflating: /content/images/candidate_images/567.png  \n",
            "  inflating: /content/images/candidate_images/568.png  \n",
            "  inflating: /content/images/candidate_images/57.png  \n",
            "  inflating: /content/images/candidate_images/570.png  \n",
            "  inflating: /content/images/candidate_images/571.png  \n",
            "  inflating: /content/images/candidate_images/572.png  \n",
            "  inflating: /content/images/candidate_images/573.png  \n",
            "  inflating: /content/images/candidate_images/574.png  \n",
            "  inflating: /content/images/candidate_images/575.png  \n",
            "  inflating: /content/images/candidate_images/577.png  \n",
            "  inflating: /content/images/candidate_images/579.png  \n",
            "  inflating: /content/images/candidate_images/58.png  \n",
            "  inflating: /content/images/candidate_images/580.png  \n",
            "  inflating: /content/images/candidate_images/581.png  \n",
            "  inflating: /content/images/candidate_images/582.png  \n",
            "  inflating: /content/images/candidate_images/583.png  \n",
            "  inflating: /content/images/candidate_images/584.png  \n",
            "  inflating: /content/images/candidate_images/586.png  \n",
            "  inflating: /content/images/candidate_images/588.png  \n",
            "  inflating: /content/images/candidate_images/589.png  \n",
            "  inflating: /content/images/candidate_images/59.png  \n",
            "  inflating: /content/images/candidate_images/590.png  \n",
            "  inflating: /content/images/candidate_images/591.png  \n",
            "  inflating: /content/images/candidate_images/592.png  \n",
            "  inflating: /content/images/candidate_images/593.png  \n",
            "  inflating: /content/images/candidate_images/594.png  \n",
            "  inflating: /content/images/candidate_images/595.png  \n",
            "  inflating: /content/images/candidate_images/598.png  \n",
            "  inflating: /content/images/candidate_images/599.png  \n",
            "  inflating: /content/images/candidate_images/6.png  \n",
            "  inflating: /content/images/candidate_images/60.png  \n",
            "  inflating: /content/images/candidate_images/600.png  \n",
            "  inflating: /content/images/candidate_images/602.png  \n",
            "  inflating: /content/images/candidate_images/603.png  \n",
            "  inflating: /content/images/candidate_images/604.png  \n",
            "  inflating: /content/images/candidate_images/605.png  \n",
            "  inflating: /content/images/candidate_images/606.png  \n",
            "  inflating: /content/images/candidate_images/607.png  \n",
            "  inflating: /content/images/candidate_images/608.png  \n",
            "  inflating: /content/images/candidate_images/609.png  \n",
            "  inflating: /content/images/candidate_images/61.png  \n",
            "  inflating: /content/images/candidate_images/610.png  \n",
            "  inflating: /content/images/candidate_images/611.png  \n",
            "  inflating: /content/images/candidate_images/612.png  \n",
            "  inflating: /content/images/candidate_images/614.png  \n",
            "  inflating: /content/images/candidate_images/615.png  \n",
            "  inflating: /content/images/candidate_images/616.png  \n",
            "  inflating: /content/images/candidate_images/617.png  \n",
            "  inflating: /content/images/candidate_images/618.png  \n",
            "  inflating: /content/images/candidate_images/619.png  \n",
            "  inflating: /content/images/candidate_images/62.png  \n",
            "  inflating: /content/images/candidate_images/620.png  \n",
            "  inflating: /content/images/candidate_images/621.png  \n",
            "  inflating: /content/images/candidate_images/624.png  \n",
            "  inflating: /content/images/candidate_images/626.png  \n",
            "  inflating: /content/images/candidate_images/628.png  \n",
            "  inflating: /content/images/candidate_images/629.png  \n",
            "  inflating: /content/images/candidate_images/630.png  \n",
            "  inflating: /content/images/candidate_images/631.png  \n",
            "  inflating: /content/images/candidate_images/632.png  \n",
            "  inflating: /content/images/candidate_images/634.png  \n",
            "  inflating: /content/images/candidate_images/635.png  \n",
            "  inflating: /content/images/candidate_images/636.png  \n",
            "  inflating: /content/images/candidate_images/637.png  \n",
            "  inflating: /content/images/candidate_images/638.png  \n",
            "  inflating: /content/images/candidate_images/639.png  \n",
            "  inflating: /content/images/candidate_images/640.png  \n",
            "  inflating: /content/images/candidate_images/641.png  \n",
            "  inflating: /content/images/candidate_images/642.png  \n",
            "  inflating: /content/images/candidate_images/643.png  \n",
            "  inflating: /content/images/candidate_images/645.png  \n",
            "  inflating: /content/images/candidate_images/646.png  \n",
            "  inflating: /content/images/candidate_images/647.png  \n",
            "  inflating: /content/images/candidate_images/648.png  \n",
            "  inflating: /content/images/candidate_images/649.png  \n",
            "  inflating: /content/images/candidate_images/65.png  \n",
            "  inflating: /content/images/candidate_images/652.png  \n",
            "  inflating: /content/images/candidate_images/654.png  \n",
            "  inflating: /content/images/candidate_images/655.png  \n",
            "  inflating: /content/images/candidate_images/656.png  \n",
            "  inflating: /content/images/candidate_images/657.png  \n",
            "  inflating: /content/images/candidate_images/658.png  \n",
            "  inflating: /content/images/candidate_images/659.png  \n",
            "  inflating: /content/images/candidate_images/66.png  \n",
            "  inflating: /content/images/candidate_images/661.png  \n",
            "  inflating: /content/images/candidate_images/662.png  \n",
            "  inflating: /content/images/candidate_images/664.png  \n",
            "  inflating: /content/images/candidate_images/665.png  \n",
            "  inflating: /content/images/candidate_images/667.png  \n",
            "  inflating: /content/images/candidate_images/668.png  \n",
            "  inflating: /content/images/candidate_images/669.png  \n",
            "  inflating: /content/images/candidate_images/67.png  \n",
            "  inflating: /content/images/candidate_images/670.png  \n",
            "  inflating: /content/images/candidate_images/673.png  \n",
            "  inflating: /content/images/candidate_images/674.png  \n",
            "  inflating: /content/images/candidate_images/675.png  \n",
            "  inflating: /content/images/candidate_images/676.png  \n",
            "  inflating: /content/images/candidate_images/677.png  \n",
            "  inflating: /content/images/candidate_images/678.png  \n",
            "  inflating: /content/images/candidate_images/679.png  \n",
            "  inflating: /content/images/candidate_images/680.png  \n",
            "  inflating: /content/images/candidate_images/682.png  \n",
            "  inflating: /content/images/candidate_images/683.png  \n",
            "  inflating: /content/images/candidate_images/684.png  \n",
            "  inflating: /content/images/candidate_images/685.png  \n",
            "  inflating: /content/images/candidate_images/686.png  \n",
            "  inflating: /content/images/candidate_images/688.png  \n",
            "  inflating: /content/images/candidate_images/689.png  \n",
            "  inflating: /content/images/candidate_images/691.png  \n",
            "  inflating: /content/images/candidate_images/692.png  \n",
            "  inflating: /content/images/candidate_images/693.png  \n",
            "  inflating: /content/images/candidate_images/695.png  \n",
            "  inflating: /content/images/candidate_images/696.png  \n",
            "  inflating: /content/images/candidate_images/699.png  \n",
            "  inflating: /content/images/candidate_images/7.png  \n",
            "  inflating: /content/images/candidate_images/700.png  \n",
            "  inflating: /content/images/candidate_images/701.png  \n",
            "  inflating: /content/images/candidate_images/703.png  \n",
            "  inflating: /content/images/candidate_images/705.png  \n",
            "  inflating: /content/images/candidate_images/706.png  \n",
            "  inflating: /content/images/candidate_images/707.png  \n",
            "  inflating: /content/images/candidate_images/71.png  \n",
            "  inflating: /content/images/candidate_images/710.png  \n",
            "  inflating: /content/images/candidate_images/711.png  \n",
            "  inflating: /content/images/candidate_images/712.png  \n",
            "  inflating: /content/images/candidate_images/714.png  \n",
            "  inflating: /content/images/candidate_images/715.png  \n",
            "  inflating: /content/images/candidate_images/716.png  \n",
            "  inflating: /content/images/candidate_images/717.png  \n",
            "  inflating: /content/images/candidate_images/719.png  \n",
            "  inflating: /content/images/candidate_images/72.png  \n",
            "  inflating: /content/images/candidate_images/720.png  \n",
            "  inflating: /content/images/candidate_images/721.png  \n",
            "  inflating: /content/images/candidate_images/722.png  \n",
            "  inflating: /content/images/candidate_images/723.png  \n",
            "  inflating: /content/images/candidate_images/724.png  \n",
            "  inflating: /content/images/candidate_images/725.png  \n",
            "  inflating: /content/images/candidate_images/727.png  \n",
            "  inflating: /content/images/candidate_images/729.png  \n",
            "  inflating: /content/images/candidate_images/73.png  \n",
            "  inflating: /content/images/candidate_images/730.png  \n",
            "  inflating: /content/images/candidate_images/731.png  \n",
            "  inflating: /content/images/candidate_images/732.png  \n",
            "  inflating: /content/images/candidate_images/735.png  \n",
            "  inflating: /content/images/candidate_images/737.png  \n",
            "  inflating: /content/images/candidate_images/739.png  \n",
            "  inflating: /content/images/candidate_images/74.png  \n",
            "  inflating: /content/images/candidate_images/741.png  \n",
            "  inflating: /content/images/candidate_images/742.png  \n",
            "  inflating: /content/images/candidate_images/743.png  \n",
            "  inflating: /content/images/candidate_images/744.png  \n",
            "  inflating: /content/images/candidate_images/745.png  \n",
            "  inflating: /content/images/candidate_images/746.png  \n",
            "  inflating: /content/images/candidate_images/748.png  \n",
            "  inflating: /content/images/candidate_images/749.png  \n",
            "  inflating: /content/images/candidate_images/750.png  \n",
            "  inflating: /content/images/candidate_images/751.png  \n",
            "  inflating: /content/images/candidate_images/752.png  \n",
            "  inflating: /content/images/candidate_images/755.png  \n",
            "  inflating: /content/images/candidate_images/756.png  \n",
            "  inflating: /content/images/candidate_images/758.png  \n",
            "  inflating: /content/images/candidate_images/759.png  \n",
            "  inflating: /content/images/candidate_images/76.png  \n",
            "  inflating: /content/images/candidate_images/761.png  \n",
            "  inflating: /content/images/candidate_images/762.png  \n",
            "  inflating: /content/images/candidate_images/763.png  \n",
            "  inflating: /content/images/candidate_images/764.png  \n",
            "  inflating: /content/images/candidate_images/766.png  \n",
            "  inflating: /content/images/candidate_images/767.png  \n",
            "  inflating: /content/images/candidate_images/768.png  \n",
            "  inflating: /content/images/candidate_images/769.png  \n",
            "  inflating: /content/images/candidate_images/770.png  \n",
            "  inflating: /content/images/candidate_images/771.png  \n",
            "  inflating: /content/images/candidate_images/772.png  \n",
            "  inflating: /content/images/candidate_images/773.png  \n",
            "  inflating: /content/images/candidate_images/775.png  \n",
            "  inflating: /content/images/candidate_images/776.png  \n",
            "  inflating: /content/images/candidate_images/778.png  \n",
            "  inflating: /content/images/candidate_images/779.png  \n",
            "  inflating: /content/images/candidate_images/78.png  \n",
            "  inflating: /content/images/candidate_images/780.png  \n",
            "  inflating: /content/images/candidate_images/781.png  \n",
            "  inflating: /content/images/candidate_images/782.png  \n",
            "  inflating: /content/images/candidate_images/783.png  \n",
            "  inflating: /content/images/candidate_images/784.png  \n",
            "  inflating: /content/images/candidate_images/785.png  \n",
            "  inflating: /content/images/candidate_images/786.png  \n",
            "  inflating: /content/images/candidate_images/787.png  \n",
            "  inflating: /content/images/candidate_images/788.png  \n",
            "  inflating: /content/images/candidate_images/789.png  \n",
            "  inflating: /content/images/candidate_images/79.png  \n",
            "  inflating: /content/images/candidate_images/790.png  \n",
            "  inflating: /content/images/candidate_images/791.png  \n",
            "  inflating: /content/images/candidate_images/792.png  \n",
            "  inflating: /content/images/candidate_images/793.png  \n",
            "  inflating: /content/images/candidate_images/794.png  \n",
            "  inflating: /content/images/candidate_images/795.png  \n",
            "  inflating: /content/images/candidate_images/797.png  \n",
            "  inflating: /content/images/candidate_images/798.png  \n",
            "  inflating: /content/images/candidate_images/799.png  \n",
            "  inflating: /content/images/candidate_images/8.png  \n",
            "  inflating: /content/images/candidate_images/80.png  \n",
            "  inflating: /content/images/candidate_images/801.png  \n",
            "  inflating: /content/images/candidate_images/803.png  \n",
            "  inflating: /content/images/candidate_images/804.png  \n",
            "  inflating: /content/images/candidate_images/805.png  \n",
            "  inflating: /content/images/candidate_images/806.png  \n",
            "  inflating: /content/images/candidate_images/807.png  \n",
            "  inflating: /content/images/candidate_images/808.png  \n",
            "  inflating: /content/images/candidate_images/809.png  \n",
            "  inflating: /content/images/candidate_images/810.png  \n",
            "  inflating: /content/images/candidate_images/811.png  \n",
            "  inflating: /content/images/candidate_images/812.png  \n",
            "  inflating: /content/images/candidate_images/813.png  \n",
            "  inflating: /content/images/candidate_images/814.png  \n",
            "  inflating: /content/images/candidate_images/815.png  \n",
            "  inflating: /content/images/candidate_images/816.png  \n",
            "  inflating: /content/images/candidate_images/817.png  \n",
            "  inflating: /content/images/candidate_images/818.png  \n",
            "  inflating: /content/images/candidate_images/819.png  \n",
            "  inflating: /content/images/candidate_images/82.png  \n",
            "  inflating: /content/images/candidate_images/820.png  \n",
            "  inflating: /content/images/candidate_images/822.png  \n",
            "  inflating: /content/images/candidate_images/823.png  \n",
            "  inflating: /content/images/candidate_images/824.png  \n",
            "  inflating: /content/images/candidate_images/825.png  \n",
            "  inflating: /content/images/candidate_images/826.png  \n",
            "  inflating: /content/images/candidate_images/828.png  \n",
            "  inflating: /content/images/candidate_images/829.png  \n",
            "  inflating: /content/images/candidate_images/83.png  \n",
            "  inflating: /content/images/candidate_images/830.png  \n",
            "  inflating: /content/images/candidate_images/833.png  \n",
            "  inflating: /content/images/candidate_images/834.png  \n",
            "  inflating: /content/images/candidate_images/836.png  \n",
            "  inflating: /content/images/candidate_images/838.png  \n",
            "  inflating: /content/images/candidate_images/84.png  \n",
            "  inflating: /content/images/candidate_images/840.png  \n",
            "  inflating: /content/images/candidate_images/842.png  \n",
            "  inflating: /content/images/candidate_images/843.png  \n",
            "  inflating: /content/images/candidate_images/844.png  \n",
            "  inflating: /content/images/candidate_images/846.png  \n",
            "  inflating: /content/images/candidate_images/847.png  \n",
            "  inflating: /content/images/candidate_images/848.png  \n",
            "  inflating: /content/images/candidate_images/85.png  \n",
            "  inflating: /content/images/candidate_images/851.png  \n",
            "  inflating: /content/images/candidate_images/852.png  \n",
            "  inflating: /content/images/candidate_images/853.png  \n",
            "  inflating: /content/images/candidate_images/854.png  \n",
            "  inflating: /content/images/candidate_images/855.png  \n",
            "  inflating: /content/images/candidate_images/856.png  \n",
            "  inflating: /content/images/candidate_images/857.png  \n",
            "  inflating: /content/images/candidate_images/858.png  \n",
            "  inflating: /content/images/candidate_images/859.png  \n",
            "  inflating: /content/images/candidate_images/86.png  \n",
            "  inflating: /content/images/candidate_images/860.png  \n",
            "  inflating: /content/images/candidate_images/861.png  \n",
            "  inflating: /content/images/candidate_images/862.png  \n",
            "  inflating: /content/images/candidate_images/863.png  \n",
            "  inflating: /content/images/candidate_images/864.png  \n",
            "  inflating: /content/images/candidate_images/865.png  \n",
            "  inflating: /content/images/candidate_images/866.png  \n",
            "  inflating: /content/images/candidate_images/867.png  \n",
            "  inflating: /content/images/candidate_images/869.png  \n",
            "  inflating: /content/images/candidate_images/87.png  \n",
            "  inflating: /content/images/candidate_images/870.png  \n",
            "  inflating: /content/images/candidate_images/871.png  \n",
            "  inflating: /content/images/candidate_images/872.png  \n",
            "  inflating: /content/images/candidate_images/873.png  \n",
            "  inflating: /content/images/candidate_images/874.png  \n",
            "  inflating: /content/images/candidate_images/875.png  \n",
            "  inflating: /content/images/candidate_images/876.png  \n",
            "  inflating: /content/images/candidate_images/878.png  \n",
            "  inflating: /content/images/candidate_images/88.png  \n",
            "  inflating: /content/images/candidate_images/882.png  \n",
            "  inflating: /content/images/candidate_images/883.png  \n",
            "  inflating: /content/images/candidate_images/885.png  \n",
            "  inflating: /content/images/candidate_images/886.png  \n",
            "  inflating: /content/images/candidate_images/887.png  \n",
            "  inflating: /content/images/candidate_images/888.png  \n",
            "  inflating: /content/images/candidate_images/889.png  \n",
            "  inflating: /content/images/candidate_images/89.png  \n",
            "  inflating: /content/images/candidate_images/891.png  \n",
            "  inflating: /content/images/candidate_images/893.png  \n",
            "  inflating: /content/images/candidate_images/894.png  \n",
            "  inflating: /content/images/candidate_images/896.png  \n",
            "  inflating: /content/images/candidate_images/897.png  \n",
            "  inflating: /content/images/candidate_images/898.png  \n",
            "  inflating: /content/images/candidate_images/899.png  \n",
            "  inflating: /content/images/candidate_images/9.png  \n",
            "  inflating: /content/images/candidate_images/90.png  \n",
            "  inflating: /content/images/candidate_images/900.png  \n",
            "  inflating: /content/images/candidate_images/901.png  \n",
            "  inflating: /content/images/candidate_images/902.png  \n",
            "  inflating: /content/images/candidate_images/903.png  \n",
            "  inflating: /content/images/candidate_images/904.png  \n",
            "  inflating: /content/images/candidate_images/905.png  \n",
            "  inflating: /content/images/candidate_images/906.png  \n",
            "  inflating: /content/images/candidate_images/908.png  \n",
            "  inflating: /content/images/candidate_images/91.png  \n",
            "  inflating: /content/images/candidate_images/910.png  \n",
            "  inflating: /content/images/candidate_images/911.png  \n",
            "  inflating: /content/images/candidate_images/912.png  \n",
            "  inflating: /content/images/candidate_images/913.png  \n",
            "  inflating: /content/images/candidate_images/915.png  \n",
            "  inflating: /content/images/candidate_images/917.png  \n",
            "  inflating: /content/images/candidate_images/918.png  \n",
            "  inflating: /content/images/candidate_images/919.png  \n",
            "  inflating: /content/images/candidate_images/92.png  \n",
            "  inflating: /content/images/candidate_images/921.png  \n",
            "  inflating: /content/images/candidate_images/922.png  \n",
            "  inflating: /content/images/candidate_images/923.png  \n",
            "  inflating: /content/images/candidate_images/925.png  \n",
            "  inflating: /content/images/candidate_images/926.png  \n",
            "  inflating: /content/images/candidate_images/927.png  \n",
            "  inflating: /content/images/candidate_images/928.png  \n",
            "  inflating: /content/images/candidate_images/929.png  \n",
            "  inflating: /content/images/candidate_images/93.png  \n",
            "  inflating: /content/images/candidate_images/930.png  \n",
            "  inflating: /content/images/candidate_images/931.png  \n",
            "  inflating: /content/images/candidate_images/932.png  \n",
            "  inflating: /content/images/candidate_images/934.png  \n",
            "  inflating: /content/images/candidate_images/936.png  \n",
            "  inflating: /content/images/candidate_images/937.png  \n",
            "  inflating: /content/images/candidate_images/940.png  \n",
            "  inflating: /content/images/candidate_images/941.png  \n",
            "  inflating: /content/images/candidate_images/942.png  \n",
            "  inflating: /content/images/candidate_images/943.png  \n",
            "  inflating: /content/images/candidate_images/944.png  \n",
            "  inflating: /content/images/candidate_images/945.png  \n",
            "  inflating: /content/images/candidate_images/946.png  \n",
            "  inflating: /content/images/candidate_images/948.png  \n",
            "  inflating: /content/images/candidate_images/95.png  \n",
            "  inflating: /content/images/candidate_images/951.png  \n",
            "  inflating: /content/images/candidate_images/952.png  \n",
            "  inflating: /content/images/candidate_images/953.png  \n",
            "  inflating: /content/images/candidate_images/954.png  \n",
            "  inflating: /content/images/candidate_images/956.png  \n",
            "  inflating: /content/images/candidate_images/957.png  \n",
            "  inflating: /content/images/candidate_images/958.png  \n",
            "  inflating: /content/images/candidate_images/959.png  \n",
            "  inflating: /content/images/candidate_images/96.png  \n",
            "  inflating: /content/images/candidate_images/960.png  \n",
            "  inflating: /content/images/candidate_images/961.png  \n",
            "  inflating: /content/images/candidate_images/962.png  \n",
            "  inflating: /content/images/candidate_images/963.png  \n",
            "  inflating: /content/images/candidate_images/964.png  \n",
            "  inflating: /content/images/candidate_images/965.png  \n",
            "  inflating: /content/images/candidate_images/968.png  \n",
            "  inflating: /content/images/candidate_images/969.png  \n",
            "  inflating: /content/images/candidate_images/97.png  \n",
            "  inflating: /content/images/candidate_images/970.png  \n",
            "  inflating: /content/images/candidate_images/971.png  \n",
            "  inflating: /content/images/candidate_images/972.png  \n",
            "  inflating: /content/images/candidate_images/973.png  \n",
            "  inflating: /content/images/candidate_images/974.png  \n",
            "  inflating: /content/images/candidate_images/977.png  \n",
            "  inflating: /content/images/candidate_images/978.png  \n",
            "  inflating: /content/images/candidate_images/979.png  \n",
            "  inflating: /content/images/candidate_images/98.png  \n",
            "  inflating: /content/images/candidate_images/980.png  \n",
            "  inflating: /content/images/candidate_images/981.png  \n",
            "  inflating: /content/images/candidate_images/982.png  \n",
            "  inflating: /content/images/candidate_images/983.png  \n",
            "  inflating: /content/images/candidate_images/984.png  \n",
            "  inflating: /content/images/candidate_images/986.png  \n",
            "  inflating: /content/images/candidate_images/987.png  \n",
            "  inflating: /content/images/candidate_images/988.png  \n",
            "  inflating: /content/images/candidate_images/989.png  \n",
            "  inflating: /content/images/candidate_images/99.png  \n",
            "  inflating: /content/images/candidate_images/990.png  \n",
            "  inflating: /content/images/candidate_images/991.png  \n",
            "  inflating: /content/images/candidate_images/992.png  \n",
            "  inflating: /content/images/candidate_images/993.png  \n",
            "  inflating: /content/images/candidate_images/994.png  \n",
            "  inflating: /content/images/candidate_images/995.png  \n",
            "  inflating: /content/images/candidate_images/996.png  \n",
            "  inflating: /content/images/candidate_images/997.png  \n",
            "  inflating: /content/images/candidate_images/998.png  \n",
            "  inflating: /content/images/candidate_images/999.png  \n",
            "  inflating: /content/images/candidate_images/candidate_descriptions.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import ViTImageProcessor, CLIPProcessor, CLIPModel\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def setup_directories():\n",
        "    \"\"\"创建必要的文件夹\"\"\"\n",
        "    os.makedirs(\"image_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/anchor_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/candidate_embeddings\", exist_ok=True)\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"从网络加载DINOv2、CLIP、ResNet101和Places365模型\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    models_dict = {}\n",
        "\n",
        "    print(\"加载DINOv2模型...\")\n",
        "    dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14', trust_repo=True).to(device).eval()\n",
        "    dinov2_processor = ViTImageProcessor.from_pretrained('facebook/dinov2-base')\n",
        "    models_dict['dinov2'] = (dinov2, dinov2_processor)\n",
        "\n",
        "    print(\"加载CLIP模型...\")\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    models_dict['clip'] = (clip_model, clip_processor)\n",
        "\n",
        "    print(\"加载ResNet101模型...\")\n",
        "    resnet101 = models.resnet101(pretrained=True).to(device).eval()\n",
        "    resnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])\n",
        "    models_dict['resnet101'] = (resnet101, None)\n",
        "\n",
        "    print(\"加载Places365模型...\")\n",
        "    try:\n",
        "        # 尝试加载Places365的ResNet50模型\n",
        "        places365 = models.resnet50(pretrained=False).to(device)\n",
        "        # 下载Places365预训练权重\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar\",\n",
        "            map_location=device\n",
        "        )\n",
        "        state_dict = {k.replace('module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
        "        places365.load_state_dict(state_dict)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "    except Exception as e:\n",
        "        print(f\"加载Places365模型失败: {e}\")\n",
        "        print(\"使用标准ResNet50作为备用...\")\n",
        "        places365 = models.resnet50(pretrained=True).to(device)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "\n",
        "    return models_dict, device\n",
        "\n",
        "def extract_id(filename):\n",
        "    \"\"\"从文件名中提取数字ID\"\"\"\n",
        "    basename = os.path.basename(filename)\n",
        "    name_without_ext = os.path.splitext(basename)[0]\n",
        "    match = re.search(r'(\\d+)', name_without_ext)\n",
        "    return int(match.group(1)) if match else float('inf')\n",
        "\n",
        "def get_sorted_images(directory):\n",
        "    \"\"\"获取指定目录下按ID排序的图像文件列表\"\"\"\n",
        "    image_files = []\n",
        "    for ext in ['*.png', '*.jpg', '*.jpeg']:\n",
        "        image_files.extend(glob.glob(os.path.join(directory, ext)))\n",
        "    return sorted(image_files, key=extract_id)\n",
        "\n",
        "def encode_images(image_files, model, processor, device, output_dir, model_name, batch_size=16):\n",
        "    \"\"\"批量编码图像并保存结果\"\"\"\n",
        "    if not image_files:\n",
        "        print(f\"警告: {output_dir} 没有找到图像文件\")\n",
        "        return\n",
        "\n",
        "    print(f\"使用 {model_name} 处理 {len(image_files)} 张图像...\")\n",
        "    all_embeddings = {}\n",
        "\n",
        "    for i in tqdm(range(0, len(image_files), batch_size)):\n",
        "        batch_files = image_files[i:i+batch_size]\n",
        "        batch_images = []\n",
        "        batch_ids = []\n",
        "\n",
        "        for img_file in batch_files:\n",
        "            try:\n",
        "                img = Image.open(img_file).convert('RGB')\n",
        "                batch_images.append(img)\n",
        "                img_id = extract_id(img_file)\n",
        "                batch_ids.append(img_id)\n",
        "            except Exception as e:\n",
        "                print(f\"处理图像 {img_file} 出错: {e}\")\n",
        "\n",
        "        if not batch_images:\n",
        "            continue\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if model_name in ['dinov2', 'clip']:\n",
        "                inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                if model_name == 'dinov2':\n",
        "                    outputs = model(**inputs)\n",
        "                    embeddings = outputs[0][:, 0, :].cpu().numpy()\n",
        "                else:\n",
        "                    outputs = model.get_image_features(**inputs)\n",
        "                    embeddings = outputs.cpu().numpy()\n",
        "            else:\n",
        "                inputs = torch.stack([\n",
        "                    transforms.ToTensor()(img.resize((224, 224))) for img in batch_images\n",
        "                ])\n",
        "                inputs = transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )(inputs).to(device)\n",
        "                embeddings = model(inputs).squeeze(-1).squeeze(-1).cpu().numpy()\n",
        "\n",
        "            normalized_embeddings = normalize(embeddings, axis=1)\n",
        "\n",
        "            for j, img_id in enumerate(batch_ids):\n",
        "                all_embeddings[img_id] = torch.tensor(normalized_embeddings[j], dtype=torch.float32)\n",
        "\n",
        "    sorted_ids = sorted(all_embeddings.keys())\n",
        "    sorted_embeddings = torch.stack([all_embeddings[img_id] for img_id in sorted_ids])\n",
        "\n",
        "    embedding_file = os.path.join(output_dir, f\"{model_name}_image_emb.pt\")\n",
        "    id_mapping_file = os.path.join(output_dir, f\"{model_name}_image_id.pt\")\n",
        "\n",
        "    torch.save(sorted_embeddings, embedding_file)\n",
        "    torch.save(sorted_ids, id_mapping_file)\n",
        "\n",
        "    print(f\"{model_name} 嵌入已保存到 {embedding_file}\")\n",
        "    print(f\"{model_name} ID映射已保存到 {id_mapping_file}\")\n",
        "    print(f\"{model_name} 嵌入形状: {sorted_embeddings.shape}\")\n",
        "\n",
        "def main():\n",
        "    setup_directories()\n",
        "    models_dict, device = load_models()\n",
        "\n",
        "    # parent_dir = os.path.dirname(os.getcwd())\n",
        "    anchor_dir = os.path.join(\"images\", \"anchor_pool\")\n",
        "    anchor_images = get_sorted_images(anchor_dir)\n",
        "    print(f\"找到 {len(anchor_images)} 张锚点图像\")\n",
        "\n",
        "    predict_dir = os.path.join(\"images\", \"candidate_images\")\n",
        "    predict_images = get_sorted_images(predict_dir)\n",
        "    print(f\"找到 {len(predict_images)} 张预测图像\")\n",
        "\n",
        "    for model_name, (model, processor) in models_dict.items():\n",
        "        encode_images(\n",
        "            anchor_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/anchor_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "        encode_images(\n",
        "            predict_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/candidate_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "\n",
        "    print(\"所有图像编码完成!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "0z23cFdQ5uPv",
        "outputId": "11d892f9-8429-40ea-e5c9-1fde273db67d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载DINOv2模型...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载CLIP模型...\n",
            "加载ResNet101模型...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Places365模型...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Places365模型失败: Error(s) in loading state_dict for ResNet:\n",
            "\tsize mismatch for fc.weight: copying a param with shape torch.Size([365, 2048]) from checkpoint, the shape in current model is torch.Size([1000, 2048]).\n",
            "\tsize mismatch for fc.bias: copying a param with shape torch.Size([365]) from checkpoint, the shape in current model is torch.Size([1000]).\n",
            "使用标准ResNet50作为备用...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "找到 203 张锚点图像\n",
            "找到 1132 张预测图像\n",
            "使用 dinov2 处理 203 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/13 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:42: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n",
            "  return self.preprocess(images, **kwargs)\n",
            "  0%|          | 0/13 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The `size` dictionary must contain the keys `height` and `width`. Got dict_keys(['shortest_edge'])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1c1973d19260>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-1c1973d19260>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         encode_images(\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0manchor_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1c1973d19260>\u001b[0m in \u001b[0;36mencode_images\u001b[0;34m(image_files, model, processor, device, output_dir, model_name, batch_size)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'dinov2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dinov2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m                 )\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvalid_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/image_processing_vit.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, do_convert_rgb)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             images = [\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/image_processing_vit.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             images = [\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/image_processing_vit.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_size_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"height\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"width\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"height\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"width\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         return resize(\n",
            "\u001b[0;31mValueError\u001b[0m: The `size` dictionary must contain the keys `height` and `width`. Got dict_keys(['shortest_edge'])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import ViTImageProcessor, CLIPProcessor, CLIPModel\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def setup_directories():\n",
        "    \"\"\"创建必要的文件夹\"\"\"\n",
        "    os.makedirs(\"image_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/anchor_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/candidate_embeddings\", exist_ok=True)\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"从网络加载DINOv2、CLIP、ResNet101和Places365模型\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    models_dict = {}\n",
        "\n",
        "    print(\"加载DINOv2模型...\")\n",
        "    dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14', trust_repo=True).to(device).eval()\n",
        "    # 显式设置size参数以避免shortest_edge问题\n",
        "    dinov2_processor = ViTImageProcessor.from_pretrained('facebook/dinov2-base', size={'height': 224, 'width': 224})\n",
        "    models_dict['dinov2'] = (dinov2, dinov2_processor)\n",
        "\n",
        "    print(\"加载CLIP模型...\")\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    models_dict['clip'] = (clip_model, clip_processor)\n",
        "\n",
        "    print(\"加载ResNet101模型...\")\n",
        "    resnet101 = models.resnet101(pretrained=True).to(device).eval()\n",
        "    resnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])\n",
        "    models_dict['resnet101'] = (resnet101, None)\n",
        "\n",
        "    print(\"加载Places365模型...\")\n",
        "    try:\n",
        "        places365 = models.resnet50(pretrained=False).to(device)\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar\",\n",
        "            map_location=device\n",
        "        )\n",
        "        state_dict = {k.replace('module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
        "        places365.load_state_dict(state_dict)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "    except Exception as e:\n",
        "        print(f\"加载Places365模型失败: {e}\")\n",
        "        print(\"使用标准ResNet50作为备用...\")\n",
        "        places365 = models.resnet50(pretrained=True).to(device)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "\n",
        "    return models_dict, device\n",
        "\n",
        "def extract_id(filename):\n",
        "    \"\"\"从文件名中提取数字ID\"\"\"\n",
        "    basename = os.path.basename(filename)\n",
        "    name_without_ext = os.path.splitext(basename)[0]\n",
        "    match = re.search(r'(\\d+)', name_without_ext)\n",
        "    return int(match.group(1)) if match else float('inf')\n",
        "\n",
        "def get_sorted_images(directory):\n",
        "    \"\"\"获取指定目录下按ID排序的图像文件列表\"\"\"\n",
        "    image_files = []\n",
        "    for ext in ['*.png', '*.jpg', '*.jpeg']:\n",
        "        image_files.extend(glob.glob(os.path.join(directory, ext)))\n",
        "    return sorted(image_files, key=extract_id)\n",
        "\n",
        "def encode_images(image_files, model, processor, device, output_dir, model_name, batch_size=16):\n",
        "    \"\"\"批量编码图像并保存结果\"\"\"\n",
        "    if not image_files:\n",
        "        print(f\"警告: {output_dir} 没有找到图像文件\")\n",
        "        return\n",
        "\n",
        "    print(f\"使用 {model_name} 处理 {len(image_files)} 张图像...\")\n",
        "    all_embeddings = {}\n",
        "\n",
        "    for i in tqdm(range(0, len(image_files), batch_size)):\n",
        "        batch_files = image_files[i:i+batch_size]\n",
        "        batch_images = []\n",
        "        batch_ids = []\n",
        "\n",
        "        for img_file in batch_files:\n",
        "            try:\n",
        "                img = Image.open(img_file).convert('RGB')\n",
        "                batch_images.append(img)\n",
        "                img_id = extract_id(img_file)\n",
        "                batch_ids.append(img_id)\n",
        "            except Exception as e:\n",
        "                print(f\"处理图像 {img_file} 出错: {e}\")\n",
        "\n",
        "        if not batch_images:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                if model_name in ['dinov2', 'clip']:\n",
        "                    # 移除padding=True，因为ViT不需要\n",
        "                    inputs = processor(images=batch_images, return_tensors=\"pt\")\n",
        "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                    if model_name == 'dinov2':\n",
        "                        outputs = model(**inputs)\n",
        "                        embeddings = outputs[0][:, 0, :].cpu().numpy()\n",
        "                    else:\n",
        "                        outputs = model.get_image_features(**inputs)\n",
        "                        embeddings = outputs.cpu().numpy()\n",
        "                else:\n",
        "                    inputs = torch.stack([\n",
        "                        transforms.ToTensor()(img.resize((224, 224))) for img in batch_images\n",
        "                    ])\n",
        "                    inputs = transforms.Normalize(\n",
        "                        mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225]\n",
        "                    )(inputs).to(device)\n",
        "                    embeddings = model(inputs).squeeze(-1).squeeze(-авис\n",
        "\n",
        "                    normalized_embeddings = normalize(embeddings, axis=1)\n",
        "\n",
        "                    for j, img_id in enumerate(batch_ids):\n",
        "                        all_embeddings[img_id] = torch.tensor(normalized_embeddings[j], dtype=torch.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"批次 {i//batch_size} 处理出错: {e}\")\n",
        "            continue\n",
        "\n",
        "    sorted_ids = sorted(all_embeddings.keys())\n",
        "    sorted_embeddings = torch.stack([all_embeddings[img_id] for img_id in sorted_ids])\n",
        "\n",
        "    embedding_file = os.path.join(output_dir, f\"{model_name}_image_emb.pt\")\n",
        "    id_mapping_file = os.path.join(output_dir, f\"{model_name}_image_id.pt\")\n",
        "\n",
        "    torch.save(sorted_embeddings, embedding_file)\n",
        "    torch.save(sorted_ids, id_mapping_file)\n",
        "\n",
        "    print(f\"{model_name} 嵌入已保存到 {embedding_file}\")\n",
        "    print(f\"{model_name} ID映射已保存到 {id_mapping_file}\")\n",
        "    print(f\"{model_name} 嵌入形状: {sorted_embeddings.shape}\")\n",
        "\n",
        "def main():\n",
        "    setup_directories()\n",
        "    models_dict, device = load_models()\n",
        "\n",
        "    parent_dir = os.path.dirname(os.getcwd())\n",
        "    anchor_dir = os.path.join(\"images\", \"anchor_pool\")\n",
        "    anchor_images = get_sorted_images(anchor_dir)\n",
        "    print(f\"找到 {len(anchor_images)} 张锚点图像\")\n",
        "\n",
        "    predict_dir = os.path.join(\"images\", \"candidate_images\")\n",
        "    predict_images = get_sorted_images(predict_dir)\n",
        "    print(f\"找到 {len(predict_images)} 张预测图像\")\n",
        "\n",
        "    for model_name, (model, processor) in models_dict.items():\n",
        "        encode_images(\n",
        "            anchor_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/anchor_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "        encode_images(\n",
        "            predict_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/candidate_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "\n",
        "    print(\"所有图像编码完成!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "1Np0R1gc73HN",
        "outputId": "290af7a2-a029-43a6-c006-bc9b0b0a4115"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'(' was never closed (<ipython-input-8-30c725b1ebc1>, line 119)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-30c725b1ebc1>\"\u001b[0;36m, line \u001b[0;32m119\u001b[0m\n\u001b[0;31m    embeddings = model(inputs).squeeze(-1).squeeze(-авис\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import ViTImageProcessor, CLIPProcessor, CLIPModel\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def setup_directories():\n",
        "    \"\"\"创建必要的文件夹\"\"\"\n",
        "    os.makedirs(\"image_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/anchor_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/candidate_embeddings\", exist_ok=True)\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"从网络加载DINOv2、CLIP、ResNet101和Places365模型\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    models_dict = {}\n",
        "\n",
        "    print(\"加载DINOv2模型...\")\n",
        "    dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14', trust_repo=True).to(device).eval()\n",
        "    dinov2_processor = ViTImageProcessor.from_pretrained('facebook/dinov2-base', size={'height': 224, 'width': 224})\n",
        "    models_dict['dinov2'] = (dinov2, dinov2_processor)\n",
        "\n",
        "    print(\"加载CLIP模型...\")\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    models_dict['clip'] = (clip_model, clip_processor)\n",
        "\n",
        "    print(\"加载ResNet101模型...\")\n",
        "    resnet101 = models.resnet101(pretrained=True).to(device).eval()\n",
        "    resnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])\n",
        "    models_dict['resnet101'] = (resnet101, None)\n",
        "\n",
        "    print(\"加载Places365模型...\")\n",
        "    try:\n",
        "        places365 = models.resnet50(pretrained=False).to(device)\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar\",\n",
        "            map_location=device\n",
        "        )\n",
        "        state_dict = {k.replace('module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
        "        places365.load_state_dict(state_dict)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "    except Exception as e:\n",
        "        print(f\"加载Places365模型失败: {e}\")\n",
        "        print(\"使用标准ResNet50作为备用...\")\n",
        "        places365 = models.resnet50(pretrained=True).to(device)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "\n",
        "    return models_dict, device\n",
        "\n",
        "def extract_id(filename):\n",
        "    \"\"\"从文件名中提取数字ID\"\"\"\n",
        "    basename = os.path.basename(filename)\n",
        "    name_without_ext = os.path.splitext(basename)[0]\n",
        "    match = re.search(r'(\\d+)', name_without_ext)\n",
        "    return int(match.group(1)) if match else float('inf')\n",
        "\n",
        "def get_sorted_images(directory):\n",
        "    \"\"\"获取指定目录下按ID排序的图像文件列表\"\"\"\n",
        "    image_files = []\n",
        "    for ext in ['*.png', '*.jpg', '*.jpeg']:\n",
        "        image_files.extend(glob.glob(os.path.join(directory, ext)))\n",
        "    return sorted(image_files, key=extract_id)\n",
        "\n",
        "def encode_images(image_files, model, processor, device, output_dir, model_name, batch_size=16):\n",
        "    \"\"\"批量编码图像并保存结果\"\"\"\n",
        "    if not image_files:\n",
        "        print(f\"警告: {output_dir} 没有找到图像文件\")\n",
        "        return\n",
        "\n",
        "    print(f\"使用 {model_name} 处理 {len(image_files)} 张图像...\")\n",
        "    all_embeddings = {}\n",
        "\n",
        "    for i in tqdm(range(0, len(image_files), batch_size)):\n",
        "        batch_files = image_files[i:i+batch_size]\n",
        "        batch_images = []\n",
        "        batch_ids = []\n",
        "\n",
        "        for img_file in batch_files:\n",
        "            try:\n",
        "                img = Image.open(img_file).convert('RGB')\n",
        "                batch_images.append(img)\n",
        "                img_id = extract_id(img_file)\n",
        "                batch_ids.append(img_id)\n",
        "            except Exception as e:\n",
        "                print(f\"处理图像 {img_file} 出错: {e}\")\n",
        "\n",
        "        if not batch_images:\n",
        "            continue\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if model_name in ['dinov2', 'clip']:\n",
        "                # 为DINOv2和CLIP移除padding参数，并确保正确处理\n",
        "                inputs = processor(images=batch_images, return_tensors=\"pt\", do_resize=True, do_normalize=True)\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                if model_name == 'dinov2':\n",
        "                    outputs = model(**inputs)\n",
        "                    embeddings = outputs[0][:, 0, :].cpu().numpy()\n",
        "                else:\n",
        "                    outputs = model.get_image_features(**inputs)\n",
        "                    embeddings = outputs.cpu().numpy()\n",
        "            else:\n",
        "                inputs = torch.stack([\n",
        "                    transforms.ToTensor()(img.resize((224, 224))) for img in batch_images\n",
        "                ])\n",
        "                inputs = transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )(inputs).to(device)\n",
        "                embeddings = model(inputs).squeeze(-1).squeeze(-1).cpu().numpy()\n",
        "\n",
        "            normalized_embeddings = normalize(embeddings, axis=1)\n",
        "\n",
        "            for j, img_id in enumerate(batch_ids):\n",
        "                all_embeddings[img_id] = torch.tensor(normalized_embeddings[j], dtype=torch.float32)\n",
        "\n",
        "    sorted_ids = sorted(all_embeddings.keys())\n",
        "    sorted_embeddings = torch.stack([all_embeddings[img_id] for img_id in sorted_ids])\n",
        "\n",
        "    embedding_file = os.path.join(output_dir, f\"{model_name}_image_emb.pt\")\n",
        "    id_mapping_file = os.path.join(output_dir, f\"{model_name}_image_id.pt\")\n",
        "\n",
        "    torch.save(sorted_embeddings, embedding_file)\n",
        "    torch.save(sorted_ids, id_mapping_file)\n",
        "\n",
        "    print(f\"{model_name} 嵌入已保存到 {embedding_file}\")\n",
        "    print(f\"{model_name} ID映射已保存到 {id_mapping_file}\")\n",
        "    print(f\"{model_name} 嵌入形状: {sorted_embeddings.shape}\")\n",
        "\n",
        "def main():\n",
        "    setup_directories()\n",
        "    models_dict, device = load_models()\n",
        "\n",
        "    parent_dir = os.path.dirname(os.getcwd())\n",
        "    anchor_dir = os.path.join(\"images\", \"anchor_pool\")\n",
        "    anchor_images = get_sorted_images(anchor_dir)\n",
        "    print(f\"找到 {len(anchor_images)} 张锚点图像\")\n",
        "\n",
        "    predict_dir = os.path.join(\"images\", \"candidate_images\")\n",
        "    predict_images = get_sorted_images(predict_dir)\n",
        "    print(f\"找到 {len(predict_images)} 张预测图像\")\n",
        "\n",
        "    for model_name, (model, processor) in models_dict.items():\n",
        "        encode_images(\n",
        "            anchor_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/anchor_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "        encode_images(\n",
        "            predict_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/candidate_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "\n",
        "    print(\"所有图像编码完成!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "fEQVbhiy9O_a",
        "outputId": "21f65c1c-a39e-4275-a432-30086b970b0b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载DINOv2模型...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载CLIP模型...\n",
            "加载ResNet101模型...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Places365模型...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Places365模型失败: Error(s) in loading state_dict for ResNet:\n",
            "\tsize mismatch for fc.weight: copying a param with shape torch.Size([365, 2048]) from checkpoint, the shape in current model is torch.Size([1000, 2048]).\n",
            "\tsize mismatch for fc.bias: copying a param with shape torch.Size([365]) from checkpoint, the shape in current model is torch.Size([1000]).\n",
            "使用标准ResNet50作为备用...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "找到 203 张锚点图像\n",
            "找到 1132 张预测图像\n",
            "使用 dinov2 处理 203 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/13 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "DinoVisionTransformer.forward_features() got an unexpected keyword argument 'pixel_values'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-43308bbf513d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-43308bbf513d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         encode_images(\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0manchor_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-43308bbf513d>\u001b[0m in \u001b[0;36mencode_images\u001b[0;34m(image_files, model, processor, device, output_dir, model_name, batch_size)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dinov2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, is_training, *args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DinoVisionTransformer.forward_features() got an unexpected keyword argument 'pixel_values'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def setup_directories():\n",
        "    \"\"\"创建必要的文件夹\"\"\"\n",
        "    os.makedirs(\"image_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/anchor_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/candidate_embeddings\", exist_ok=True)\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"从网络加载DINOv2、CLIP、ResNet101和Places365模型\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    models_dict = {}\n",
        "\n",
        "    print(\"加载DINOv2模型...\")\n",
        "    dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14', trust_repo=True).to(device).eval()\n",
        "    # DINOv2需要自定义预处理，不使用ViTImageProcessor\n",
        "    dinov2_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    models_dict['dinov2'] = (dinov2, dinov2_transform)\n",
        "\n",
        "    print(\"加载CLIP模型...\")\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    models_dict['clip'] = (clip_model, clip_processor)\n",
        "\n",
        "    print(\"加载ResNet101模型...\")\n",
        "    resnet101 = models.resnet101(pretrained=True).to(device).eval()\n",
        "    resnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])\n",
        "    models_dict['resnet101'] = (resnet101, None)\n",
        "\n",
        "    print(\"加载Places365模型...\")\n",
        "    try:\n",
        "        places365 = models.resnet50(pretrained=False).to(device)\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar\",\n",
        "            map_location=device\n",
        "        )\n",
        "        state_dict = {k.replace('module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
        "        places365.load_state_dict(state_dict)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "    except Exception as e:\n",
        "        print(f\"加载Places365模型失败: {e}\")\n",
        "        print(\"使用标准ResNet50作为备用...\")\n",
        "        places365 = models.resnet50(pretrained=True).to(device)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "\n",
        "    return models_dict, device\n",
        "\n",
        "def extract_id(filename):\n",
        "    \"\"\"从文件名中提取数字ID\"\"\"\n",
        "    basename = os.path.basename(filename)\n",
        "    name_without_ext = os.path.splitext(basename)[0]\n",
        "    match = re.search(r'(\\d+)', name_without_ext)\n",
        "    return int(match.group(1)) if match else float('inf')\n",
        "\n",
        "def get_sorted_images(directory):\n",
        "    \"\"\"获取指定目录下按ID排序的图像文件列表\"\"\"\n",
        "    image_files = []\n",
        "    for ext in ['*.png', '*.jpg', '*.jpeg']:\n",
        "        image_files.extend(glob.glob(os.path.join(directory, ext)))\n",
        "    return sorted(image_files, key=extract_id)\n",
        "\n",
        "def encode_images(image_files, model, processor, device, output_dir, model_name, batch_size=16):\n",
        "    \"\"\"批量编码图像并保存结果\"\"\"\n",
        "    if not image_files:\n",
        "        print(f\"警告: {output_dir} 没有找到图像文件\")\n",
        "        return\n",
        "\n",
        "    print(f\"使用 {model_name} 处理 {len(image_files)} 张图像...\")\n",
        "    all_embeddings = {}\n",
        "\n",
        "    for i in tqdm(range(0, len(image_files), batch_size)):\n",
        "        batch_files = image_files[i:i+batch_size]\n",
        "        batch_images = []\n",
        "        batch_ids = []\n",
        "\n",
        "        for img_file in batch_files:\n",
        "            try:\n",
        "                img = Image.open(img_file).convert('RGB')\n",
        "                batch_images.append(img)\n",
        "                img_id = extract_id(img_file)\n",
        "                batch_ids.append(img_id)\n",
        "            except Exception as e:\n",
        "                print(f\"处理图像 {img_file} 出错: {e}\")\n",
        "\n",
        "        if not batch_images:\n",
        "            continue\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if model_name == 'dinov2':\n",
        "                # DINOv2使用torchvision的transform处理\n",
        "                inputs = torch.stack([processor(img) for img in batch_images]).to(device)\n",
        "                outputs = model(inputs)\n",
        "                embeddings = outputs[:, 0, :].cpu().numpy()  # 获取[CLS] token\n",
        "            elif model_name == 'clip':\n",
        "                inputs = processor(images=batch_images, return_tensors=\"pt\", do_resize=True, do_normalize=True)\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                outputs = model.get_image_features(**inputs)\n",
        "                embeddings = outputs.cpu().numpy()\n",
        "            else:\n",
        "                inputs = torch.stack([\n",
        "                    transforms.ToTensor()(img.resize((224, 224))) for img in batch_images\n",
        "                ])\n",
        "                inputs = transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )(inputs).to(device)\n",
        "                embeddings = model(inputs).squeeze(-1).squeeze(-1).cpu().numpy()\n",
        "\n",
        "            normalized_embeddings = normalize(embeddings, axis=1)\n",
        "\n",
        "            for j, img_id in enumerate(batch_ids):\n",
        "                all_embeddings[img_id] = torch.tensor(normalized_embeddings[j], dtype=torch.float32)\n",
        "\n",
        "    sorted_ids = sorted(all_embeddings.keys())\n",
        "    sorted_embeddings = torch.stack([all_embeddings[img_id] for img_id in sorted_ids])\n",
        "\n",
        "    embedding_file = os.path.join(output_dir, f\"{model_name}_image_emb.pt\")\n",
        "    id_mapping_file = os.path.join(output_dir, f\"{model_name}_image_id.pt\")\n",
        "\n",
        "    torch.save(sorted_embeddings, embedding_file)\n",
        "    torch.save(sorted_ids, id_mapping_file)\n",
        "\n",
        "    print(f\"{model_name} 嵌入已保存到 {embedding_file}\")\n",
        "    print(f\"{model_name} ID映射已保存到 {id_mapping_file}\")\n",
        "    print(f\"{model_name} 嵌入形状: {sorted_embeddings.shape}\")\n",
        "\n",
        "def main():\n",
        "    setup_directories()\n",
        "    models_dict, device = load_models()\n",
        "\n",
        "    parent_dir = os.path.dirname(os.getcwd())\n",
        "    anchor_dir = os.path.join(\"images\", \"anchor_pool\")\n",
        "    anchor_images = get_sorted_images(anchor_dir)\n",
        "    print(f\"找到 {len(anchor_images)} 张锚点图像\")\n",
        "\n",
        "    predict_dir = os.path.join(\"images\", \"candidate_images\")\n",
        "    predict_images = get_sorted_images(predict_dir)\n",
        "    print(f\"找到 {len(predict_images)} 张预测图像\")\n",
        "\n",
        "    for model_name, (model, processor) in models_dict.items():\n",
        "        encode_images(\n",
        "            anchor_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/anchor_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "        encode_images(\n",
        "            predict_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/candidate_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "\n",
        "    print(\"所有图像编码完成!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "QVfk1gFL9zm-",
        "outputId": "9540465a-6f1e-4473-e060-160a0b534c2e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载DINOv2模型...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载CLIP模型...\n",
            "加载ResNet101模型...\n",
            "加载Places365模型...\n",
            "加载Places365模型失败: Error(s) in loading state_dict for ResNet:\n",
            "\tsize mismatch for fc.weight: copying a param with shape torch.Size([365, 2048]) from checkpoint, the shape in current model is torch.Size([1000, 2048]).\n",
            "\tsize mismatch for fc.bias: copying a param with shape torch.Size([365]) from checkpoint, the shape in current model is torch.Size([1000]).\n",
            "使用标准ResNet50作为备用...\n",
            "找到 203 张锚点图像\n",
            "找到 1132 张预测图像\n",
            "使用 dinov2 处理 203 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/13 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9f5a32c3edbf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-9f5a32c3edbf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         encode_images(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0manchor_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-9f5a32c3edbf>\u001b[0m in \u001b[0;36mencode_images\u001b[0;34m(image_files, model, processor, device, output_dir, model_name, batch_size)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取[CLS] token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def setup_directories():\n",
        "    \"\"\"创建必要的文件夹\"\"\"\n",
        "    os.makedirs(\"image_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/anchor_embeddings\", exist_ok=True)\n",
        "    os.makedirs(\"image_embeddings/candidate_embeddings\", exist_ok=True)\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"从网络加载DINOv2、CLIP、ResNet101和Places365模型\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    models_dict = {}\n",
        "\n",
        "    print(\"加载DINOv2模型...\")\n",
        "    dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14', trust_repo=True).to(device).eval()\n",
        "    dinov2_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    models_dict['dinov2'] = (dinov2, dinov2_transform)\n",
        "\n",
        "    print(\"加载CLIP模型...\")\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    models_dict['clip'] = (clip_model, clip_processor)\n",
        "\n",
        "    print(\"加载ResNet101模型...\")\n",
        "    resnet101 = models.resnet101(pretrained=True).to(device).eval()\n",
        "    resnet101 = torch.nn.Sequential(*list(resnet101.children())[:-1])\n",
        "    models_dict['resnet101'] = (resnet101, None)\n",
        "\n",
        "    print(\"加载Places365模型...\")\n",
        "    try:\n",
        "        places365 = models.resnet50(pretrained=False).to(device)\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar\",\n",
        "            map_location=device\n",
        "        )\n",
        "        state_dict = {k.replace('module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
        "        places365.load_state_dict(state_dict)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "    except Exception as e:\n",
        "        print(f\"加载Places365模型失败: {e}\")\n",
        "        print(\"使用标准ResNet50作为备用...\")\n",
        "        places365 = models.resnet50(pretrained=True).to(device)\n",
        "        places365 = torch.nn.Sequential(*list(places365.children())[:-1]).eval()\n",
        "        models_dict['places365'] = (places365, None)\n",
        "\n",
        "    return models_dict, device\n",
        "\n",
        "def extract_id(filename):\n",
        "    \"\"\"从文件名中提取数字ID\"\"\"\n",
        "    basename = os.path.basename(filename)\n",
        "    name_without_ext = os.path.splitext(basename)[0]\n",
        "    match = re.search(r'(\\d+)', name_without_ext)\n",
        "    return int(match.group(1)) if match else float('inf')\n",
        "\n",
        "def get_sorted_images(directory):\n",
        "    \"\"\"获取指定目录下按ID排序的图像文件列表\"\"\"\n",
        "    image_files = []\n",
        "    for ext in ['*.png', '*.jpg', '*.jpeg']:\n",
        "        image_files.extend(glob.glob(os.path.join(directory, ext)))\n",
        "    return sorted(image_files, key=extract_id)\n",
        "\n",
        "def encode_images(image_files, model, processor, device, output_dir, model_name, batch_size=16):\n",
        "    \"\"\"批量编码图像并保存结果\"\"\"\n",
        "    if not image_files:\n",
        "        print(f\"警告: {output_dir} 没有找到图像文件\")\n",
        "        return\n",
        "\n",
        "    print(f\"使用 {model_name} 处理 {len(image_files)} 张图像...\")\n",
        "    all_embeddings = {}\n",
        "\n",
        "    for i in tqdm(range(0, len(image_files), batch_size)):\n",
        "        batch_files = image_files[i:i+batch_size]\n",
        "        batch_images = []\n",
        "        batch_ids = []\n",
        "\n",
        "        for img_file in batch_files:\n",
        "            try:\n",
        "                img = Image.open(img_file).convert('RGB')\n",
        "                batch_images.append(img)\n",
        "                img_id = extract_id(img_file)\n",
        "                batch_ids.append(img_id)\n",
        "            except Exception as e:\n",
        "                print(f\"处理图像 {img_file} 出错: {e}\")\n",
        "\n",
        "        if not batch_images:\n",
        "            continue\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if model_name == 'dinov2':\n",
        "                inputs = torch.stack([processor(img) for img in batch_images]).to(device)\n",
        "                outputs = model(inputs)\n",
        "                embeddings = outputs.cpu().numpy()  # DINOv2直接输出[batch_size, embedding_dim]\n",
        "            elif model_name == 'clip':\n",
        "                inputs = processor(images=batch_images, return_tensors=\"pt\", do_resize=True, do_normalize=True)\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                outputs = model.get_image_features(**inputs)\n",
        "                embeddings = outputs.cpu().numpy()\n",
        "            else:\n",
        "                inputs = torch.stack([\n",
        "                    transforms.ToTensor()(img.resize((224, 224))) for img in batch_images\n",
        "                ])\n",
        "                inputs = transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )(inputs).to(device)\n",
        "                embeddings = model(inputs).squeeze(-1).squeeze(-1).cpu().numpy()\n",
        "\n",
        "            normalized_embeddings = normalize(embeddings, axis=1)\n",
        "\n",
        "            for j, img_id in enumerate(batch_ids):\n",
        "                all_embeddings[img_id] = torch.tensor(normalized_embeddings[j], dtype=torch.float32)\n",
        "\n",
        "    sorted_ids = sorted(all_embeddings.keys())\n",
        "    sorted_embeddings = torch.stack([all_embeddings[img_id] for img_id in sorted_ids])\n",
        "\n",
        "    embedding_file = os.path.join(output_dir, f\"{model_name}_image_emb.pt\")\n",
        "    id_mapping_file = os.path.join(output_dir, f\"{model_name}_image_id.pt\")\n",
        "\n",
        "    torch.save(sorted_embeddings, embedding_file)\n",
        "    torch.save(sorted_ids, id_mapping_file)\n",
        "\n",
        "    print(f\"{model_name} 嵌入已保存到 {embedding_file}\")\n",
        "    print(f\"{model_name} ID映射已保存到 {id_mapping_file}\")\n",
        "    print(f\"{model_name} 嵌入形状: {sorted_embeddings.shape}\")\n",
        "\n",
        "def main():\n",
        "    setup_directories()\n",
        "    models_dict, device = load_models()\n",
        "\n",
        "    parent_dir = os.path.dirname(os.getcwd())\n",
        "    anchor_dir = os.path.join(\"images\", \"anchor_pool\")\n",
        "    anchor_images = get_sorted_images(anchor_dir)\n",
        "    print(f\"找到 {len(anchor_images)} 张锚点图像\")\n",
        "\n",
        "    predict_dir = os.path.join(\"images\", \"candidate_images\")\n",
        "    predict_images = get_sorted_images(predict_dir)\n",
        "    print(f\"找到 {len(predict_images)} 张预测图像\")\n",
        "\n",
        "    for model_name, (model, processor) in models_dict.items():\n",
        "        encode_images(\n",
        "            anchor_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/anchor_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "        encode_images(\n",
        "            predict_images,\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            \"image_embeddings/candidate_embeddings\",\n",
        "            model_name\n",
        "        )\n",
        "\n",
        "    print(\"所有图像编码完成!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr-vqgwe-UAe",
        "outputId": "0b871a3a-3f35-4d3d-b1f8-4112d871083c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载DINOv2模型...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载CLIP模型...\n",
            "加载ResNet101模型...\n",
            "加载Places365模型...\n",
            "加载Places365模型失败: Error(s) in loading state_dict for ResNet:\n",
            "\tsize mismatch for fc.weight: copying a param with shape torch.Size([365, 2048]) from checkpoint, the shape in current model is torch.Size([1000, 2048]).\n",
            "\tsize mismatch for fc.bias: copying a param with shape torch.Size([365]) from checkpoint, the shape in current model is torch.Size([1000]).\n",
            "使用标准ResNet50作为备用...\n",
            "找到 203 张锚点图像\n",
            "找到 1132 张预测图像\n",
            "使用 dinov2 处理 203 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:02<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dinov2 嵌入已保存到 image_embeddings/anchor_embeddings/dinov2_image_emb.pt\n",
            "dinov2 ID映射已保存到 image_embeddings/anchor_embeddings/dinov2_image_id.pt\n",
            "dinov2 嵌入形状: torch.Size([203, 768])\n",
            "使用 dinov2 处理 1132 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 71/71 [00:16<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dinov2 嵌入已保存到 image_embeddings/candidate_embeddings/dinov2_image_emb.pt\n",
            "dinov2 ID映射已保存到 image_embeddings/candidate_embeddings/dinov2_image_id.pt\n",
            "dinov2 嵌入形状: torch.Size([1132, 768])\n",
            "使用 clip 处理 203 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:02<00:00,  5.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clip 嵌入已保存到 image_embeddings/anchor_embeddings/clip_image_emb.pt\n",
            "clip ID映射已保存到 image_embeddings/anchor_embeddings/clip_image_id.pt\n",
            "clip 嵌入形状: torch.Size([203, 512])\n",
            "使用 clip 处理 1132 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 71/71 [00:12<00:00,  5.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clip 嵌入已保存到 image_embeddings/candidate_embeddings/clip_image_emb.pt\n",
            "clip ID映射已保存到 image_embeddings/candidate_embeddings/clip_image_id.pt\n",
            "clip 嵌入形状: torch.Size([1132, 512])\n",
            "使用 resnet101 处理 203 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:02<00:00,  5.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet101 嵌入已保存到 image_embeddings/anchor_embeddings/resnet101_image_emb.pt\n",
            "resnet101 ID映射已保存到 image_embeddings/anchor_embeddings/resnet101_image_id.pt\n",
            "resnet101 嵌入形状: torch.Size([203, 2048])\n",
            "使用 resnet101 处理 1132 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 71/71 [00:11<00:00,  6.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet101 嵌入已保存到 image_embeddings/candidate_embeddings/resnet101_image_emb.pt\n",
            "resnet101 ID映射已保存到 image_embeddings/candidate_embeddings/resnet101_image_id.pt\n",
            "resnet101 嵌入形状: torch.Size([1132, 2048])\n",
            "使用 places365 处理 203 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  6.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "places365 嵌入已保存到 image_embeddings/anchor_embeddings/places365_image_emb.pt\n",
            "places365 ID映射已保存到 image_embeddings/anchor_embeddings/places365_image_id.pt\n",
            "places365 嵌入形状: torch.Size([203, 2048])\n",
            "使用 places365 处理 1132 张图像...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 71/71 [00:10<00:00,  6.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "places365 嵌入已保存到 image_embeddings/candidate_embeddings/places365_image_emb.pt\n",
            "places365 ID映射已保存到 image_embeddings/candidate_embeddings/places365_image_id.pt\n",
            "places365 嵌入形状: torch.Size([1132, 2048])\n",
            "所有图像编码完成!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 移动 /content/images/image_embeddings 文件夹下的文件到 Google Drive 的指定路径\n",
        "# 假设目标路径为 Google Drive 的 MyDrive/images/image_embeddings\n",
        "!mkdir -p /content/drive/MyDrive/images/image_embeddings\n",
        "!mv /content/image_embeddings/* /content/drive/MyDrive/images/image_embeddings/"
      ],
      "metadata": {
        "id": "lVX4U1Xo_L6I"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}